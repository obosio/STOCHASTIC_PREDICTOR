\documentclass[11pt, a4paper]{report}

% --- PREAMBLE ---
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{fontspec}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{amsthm}

\usepackage[english]{babel}

% Environments
\newtheorem{testcase}{Test Case}[chapter]
\newtheorem{implementation}{Implementation}[chapter]
\newtheorem{criterion}{Criterion}[chapter]

% --- HYPERREF (must be last) ---
\usepackage[hidelinks]{hyperref}

\title{\textbf{Python Test Policies \\ for the Universal Stochastic Predictor}}
\author{Adaptive Meta-Prediction Development Consortium}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\chapter{Purpose and Scope}

\section{Objective}

The test suite must validate that the system is correct, stable, reproducible, and safe under the operational conditions
defined by the specification. The objective is to prevent regressions, protect numerical integrity, and enforce strict
quality policies across all layers.

\section{Scope}

Tests must cover the API, Core, Kernel, and I/O layers, including integration boundaries and cross-layer contracts.
Coverage includes deterministic functionality, numerical invariants, error handling, and operational edge cases.

\chapter{Quality Expectations}

\section{Code Quality}

All production and test code must follow consistent formatting, linting rules, and type safety constraints. The test
policy requires deterministic behavior, explicit error handling, and no unused or dead code in critical paths.

\section{Configuration Discipline}

No hardcoded parameters are allowed in production logic. All tunable values must be driven by configuration. Tests must
verify configuration loading, validation, and propagation, including defaults and boundary constraints.

\section{No Fallback Policy}

Silent fallbacks are forbidden. If a dependency, configuration, or invariant is missing or invalid, the system must fail
fast with an explicit, actionable error. Tests must assert this behavior.

\section{Security and Secrets}

No secrets or credentials may be hardcoded. Tests must verify that secret material is injected only through authorized
channels and never appears in logs, reports, or snapshots.

\chapter{Completeness Criteria}

\section{Coverage}

Each layer must have unit tests for core logic, integration tests for inter-layer contracts, and robustness tests for
stress conditions. The test suite must include negative cases for invalid inputs and explicit failure modes.

\section{Public API Surface}

All public callables must have at least one validation path that confirms correct execution and error handling. Public
contracts must be verified for input validation, output structure, and invariants.

\chapter{Performance and Robustness}

\section{Performance Targets}

Performance-sensitive components must meet documented latency and throughput thresholds under controlled inputs. Tests
must report measured values and enforce explicit acceptance thresholds.

\section{Robustness Targets}

The system must remain stable under heavy tails, outliers, regime shifts, and numerical stress. Tests must quantify
false-positive and false-negative rates for detectors and enforce stability criteria.

\section{Determinism and Reproducibility}

All tests must be deterministic under fixed seeds and configuration. Each run must record configuration, environment
metadata, and timing so that results can be reproduced.

\chapter{Execution Policies}

\section{Isolation}

Tests must run in an isolated environment with locked dependencies. No system-wide packages may influence results.
The environment must be reproducible across supported platforms.

\section{Tiered Execution}

Unit, integration, robustness, I/O, hardware, validation, and edge-case tiers must be independently executable.
Each tier must define clear entry and exit criteria.

\chapter{Reporting Requirements}

Reports must summarize pass/fail status by tier, list missing dependencies, and provide lint and type statistics.
Reports must be structured, versioned, and reproducible.

\chapter{Acceptance Criteria}

The suite is accepted when:
\begin{itemize}
    \item All required tiers pass in the baseline environment
    \item No blocking issues remain in formatting, linting, or type checks
    \item Dependency validation reports zero missing required packages
    \item Critical numerical invariants meet documented thresholds
    \item Performance targets meet or exceed acceptance thresholds
\end{itemize}

\end{document}\documentclass[11pt, a4paper]{report}

% --- PREAMBLE ---
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{fontspec}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{amsthm}

\usepackage[english]{babel}

% Environments
\newtheorem{testcase}{Test Case}[chapter]
\newtheorem{implementation}{Implementation}[chapter]
\newtheorem{criterion}{Criterion}[chapter]

% --- HYPERREF (must be last) ---
\usepackage[hidelinks]{hyperref}

\title{\textbf{Python Test Suite Policies \\ for the Universal Stochastic Predictor}}
\author{Adaptive Meta-Prediction Development Consortium}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\chapter{Testing Environment Setup}

\section{Dependency Governance}

The test environment must define a locked dependency set with exact versions.
Dependencies are separated by layer (production vs testing) and must be installed in an isolated environment.
All test utilities and diagnostics required for execution must be declared in the test dependency set.

\section{Environment Isolation}

Tests must run in an isolated environment with no reliance on system-wide packages.
The environment must be reproducible across supported platforms.

\section{Tooling-Neutral Checks}

The test workflow must include:
\begin{itemize}
    \item Formatting validation
    \item Linting and static analysis
    \item Type checking
    \item Dependency validation against the installed environment
\end{itemize}

\chapter{Test Taxonomy and Coverage}

\section{Unit Tests}

Unit tests validate deterministic behavior of isolated algorithms and must not rely on global system state.
They must include numerical invariants, boundary conditions, and error handling.

\section{Integration Tests}

Integration tests validate cross-module interactions and data flow contracts.
They must include end-to-end execution of kernel orchestration with controlled inputs.

\section{Robustness Tests}

Robustness tests validate stability under heavy tails, outliers, regime shifts, and numerical stress.
They must include false-positive and false-negative analysis for detectors.

\section{I/O Tests}

I/O tests validate snapshot persistence, telemetry integrity, and recovery procedures.
They must include atomicity and corruption-resistance criteria.

\section{Hardware Consistency Tests}

Hardware tests validate numerical parity across supported devices.
They must quantify tolerated drift and enforce reproducibility within defined thresholds.

\section{Validation and Edge Cases}

Validation tests enforce causal correctness, windowed evaluation policies, and degraded-mode recovery.
Edge-case tests must exercise time-to-live violations, degenerate inputs, and mode-collapse detection.

\chapter{Execution Policies}

All test executions must be deterministic under fixed seeds and must log configuration, environment, and timing.
Tests may be skipped only when explicitly justified by unavailable optional resources.

\chapter{Reporting Requirements}

Reports must summarize pass/fail status by tier, list missing dependencies, and provide lint/type statistics.
Reports must be generated in a structured, versioned format and must be reproducible.

\chapter{Acceptance Criteria}

The test suite is accepted when:
\begin{itemize}
    \item All required tiers pass in the baseline environment
    \item No blocking issues remain in lint or static analysis
    \item Dependency validation reports zero missing required packages
    \item Critical numerical invariants meet documented thresholds
\end{itemize}

\end{document}\documentclass[11pt, a4paper]{report}

% --- PREAMBLE ---
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{fontspec}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{xcolor}

\usepackage[english]{babel}

% Environments
\newtheorem{testcase}{Test Case}[chapter]
\newtheorem{implementation}{Implementation}[chapter]
\newtheorem{criterion}{Criterion}[chapter]

% Listings configuration
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=pythonstyle, language=Python}

% --- HYPERREF (must be last) ---
\usepackage[hidelinks]{hyperref}

\title{\textbf{Python Test Suite \\ for the Universal Stochastic Predictor}}
\author{Adaptive Meta-Prediction Development Consortium}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\chapter{Testing Environment Setup}

\section{Dependency Governance}

The test environment must define a locked dependency set with exact versions.
Dependencies are separated by layer (production vs testing) and must be installed in an isolated environment.
All test utilities and diagnostics required for execution must be declared in the test dependency set.

\section{Directory Structure Policy}

The test hierarchy must separate unit, integration, robustness, I/O, hardware, validation, and edge-case coverage.
Each category must be independently executable and must not rely on implicit ordering.

\section{Shared Fixtures Policy}

Shared fixtures must be centralized, deterministic, and reusable across all test tiers.
Fixtures must avoid hidden state and must document any randomness or external dependencies.

\section{Dependency and Static Analysis Policies}

Dependency validation compares imported modules against declared requirements and the installed environment.
Static analysis enforces consistent formatting, linting rules, and type checking.

\chapter{Test Taxonomy and Coverage}

\section{Unit Tests}

Unit tests validate deterministic behavior of isolated algorithms and must not rely on global system state.
They must include numerical invariants, boundary conditions, and error handling.

\section{Integration Tests}

Integration tests validate cross-module interactions and data flow contracts.
They must include end-to-end execution of kernel orchestration with controlled inputs.

\section{Robustness Tests}

Robustness tests validate stability under heavy tails, outliers, regime shifts, and numerical stress.
They must include false-positive and false-negative analysis for detectors.

\section{I/O Tests}

I/O tests validate snapshot persistence, telemetry integrity, and recovery procedures.
They must include atomicity and corruption-resistance criteria.

\section{Hardware Consistency Tests}

Hardware tests validate numerical parity across supported devices.
They must quantify tolerated drift and enforce reproducibility within defined thresholds.

\section{Validation and Edge Cases}

Validation tests enforce causal correctness, windowed evaluation policies, and degraded-mode recovery.
Edge-case tests must exercise time-to-live violations, degenerate inputs, and mode-collapse detection.

\chapter{Execution Policies}

All test executions must be deterministic under fixed seeds and must log configuration, environment, and timing.
Tests may be skipped only when explicitly justified by unavailable optional resources.

\chapter{Reporting Requirements}

Reports must summarize pass/fail status by tier, list missing dependencies, and provide lint/type statistics.
Reports must be generated in a structured, versioned format and must be reproducible.

\chapter{Acceptance Criteria}

The test suite is accepted when:
\begin{itemize}
    \item All required tiers pass in the baseline environment
    \item No blocking issues remain in lint or static analysis
    \item Dependency validation reports zero missing required packages
    \item Critical numerical invariants meet documented thresholds
\end{itemize}

\end{document}

    weights_prev = jnp.array([0.25, 0.25, 0.25, 0.25])
    gradients = jnp.array([0.1, -0.2, 0.05, -0.1])

    weights_new = jko.solve_ot_step(weights_prev, gradients, tau=0.1)

    assert jnp.abs(jnp.sum(weights_new) - 1.0) < 1e-8, \
        "Simplex constraint violated"

    assert jnp.all(weights_new >= 0), "Negative weights detected"

def test_jko_energy_descent():
    """
    Test: JKO must reduce energy along gradient direction.
    """
    jko = JKO_Discreto(epsilon=1e-2)

    weights_prev = jnp.array([0.5, 0.2, 0.2, 0.1])
    gradients = jnp.array([1.0, -0.5, -0.3, -0.2])

    weights_new = jko.solve_ot_step(weights_prev, gradients, tau=0.1)

    assert weights_new[0] < weights_prev[0], \
        f"JKO did not reduce high-energy kernel: " \
        f"{weights_new[0]:.3f} vs {weights_prev[0]:.3f}"
\end{lstlisting}

\chapter{I/O and Persistence Tests}

\section{Atomic Snapshotting Test}

\begin{lstlisting}
# tests/test_io/test_snapshotting.py
import pytest
import tempfile
import os
from Python.predictor import UniversalPredictor
from Python.config import PredictorConfig

def test_snapshot_save_load_integrity():
    """
    Test: Snapshot must preserve full state with checksum.
    """
    config = PredictorConfig()
    predictor1 = UniversalPredictor(config)

    for _ in range(50):
        obs = 100.0 + np.random.randn()
        predictor1.step_with_telemetry(obs, previous_target=obs)

    with tempfile.NamedTemporaryFile(delete=False, suffix='.msgpack') as f:
        filepath = f.name

    try:
        predictor1.save_snapshot(filepath)

        predictor2 = UniversalPredictor(config)
        predictor2.load_snapshot(filepath)

        result1 = predictor1.step_with_telemetry(
            105.0, previous_target=105.0
        )
        result2 = predictor2.step_with_telemetry(
            105.0, previous_target=105.0
        )

        assert jnp.allclose(result1.weights, result2.weights, atol=1e-6), \
            "Weights mismatch after snapshot restore"

        assert jnp.allclose(
            result1.holder_exponent, result2.holder_exponent, atol=1e-6
        ), "Holder exponent mismatch"

    finally:
        os.unlink(filepath)

def test_snapshot_corruption_detection():
    """
    Test: Corrupted snapshot must be rejected.
    """
    config = PredictorConfig()
    predictor1 = UniversalPredictor(config)

    with tempfile.NamedTemporaryFile(delete=False, suffix='.msgpack') as f:
        filepath = f.name

    try:
        predictor1.save_snapshot(filepath)

        with open(filepath, 'rb+') as f:
            f.seek(100)
            f.write(b'\x00\x00\x00\x00')

        predictor2 = UniversalPredictor(config)

        with pytest.raises(ValueError, match="Checksum mismatch"):
            predictor2.load_snapshot(filepath)

    finally:
        os.unlink(filepath)

def test_snapshot_includes_telemetry():
    """
    Test: Snapshot must include kurtosis, DGM entropy, and flags.
    """
    import msgpack

    config = PredictorConfig()
    predictor = UniversalPredictor(config)

    for _ in range(300):
        obs = 100.0 + np.random.randn() * 5.0
        predictor.step_with_telemetry(obs, previous_target=obs)

    with tempfile.NamedTemporaryFile(delete=False, suffix='.msgpack') as f:
        filepath = f.name

    try:
        predictor.save_snapshot(filepath)

        with open(filepath, 'rb') as f:
            content = f.read()

        data_bytes = content[:-64]
        payload = msgpack.unpackb(data_bytes)

        assert 'telemetry' in payload, "Telemetry missing from snapshot"
        assert 'kurtosis' in payload['telemetry'], "Kurtosis not saved"
        assert 'dgm_entropy' in payload['telemetry'], "DGM entropy not saved"

        assert 'flags' in payload, "Flags missing from snapshot"
        assert 'degraded_inference' in payload['flags']
        assert 'emergency' in payload['flags']
        assert 'regime_change' in payload['flags']
        assert 'mode_collapse' in payload['flags']

    finally:
        os.unlink(filepath)
\end{lstlisting}

\chapter{Hardware Tests: CPU/GPU Parity}

\section{Numerical Consistency Test}

\begin{lstlisting}
# tests/test_hardware/test_cpu_gpu_parity.py
import pytest
import jax
import jax.numpy as jnp
from Python.predictor import UniversalPredictor
from Python.config import PredictorConfig

@pytest.mark.parametrize("device", ["cpu", "gpu"])
def test_device_consistency(device):
    """
    Test: CPU and GPU must produce equivalent results.
    """
    if device == "gpu" and not jax.devices('gpu'):
        pytest.skip("GPU not available")

    with jax.default_device(jax.devices(device)[0]):
        config = PredictorConfig()
        predictor = UniversalPredictor(config)

        np.random.seed(555)
        data = np.random.randn(100) * 10.0 + 100.0

        results = []
        for obs in data:
            result = predictor.step_with_telemetry(obs, previous_target=obs)
            results.append({
                'prediction': float(result.predicted_next),
                'holder': float(result.holder_exponent),
                'weights': result.weights
            })

        return results

def test_cpu_gpu_parity():
    """
    Test: Compare CPU and GPU results.
    """
    if not jax.devices('gpu'):
        pytest.skip("GPU not available for parity test")

    results_cpu = test_device_consistency("cpu")
    results_gpu = test_device_consistency("gpu")

    for i, (cpu, gpu) in enumerate(zip(results_cpu, results_gpu)):
        assert jnp.allclose(
            cpu['weights'], gpu['weights'], rtol=1e-5, atol=1e-6
        ), f"Weights mismatch at step {i}"

        pred_diff = abs(cpu['prediction'] - gpu['prediction'])
        assert pred_diff < 1e-4, \
            f"Prediction mismatch at step {i}: {pred_diff:.2e}"
\end{lstlisting}

\section{Hardware Parity with Quantization (FPGA Simulation)}

\begin{lstlisting}
# tests/test_hardware/test_fixed_point_parity.py
import pytest
import jax.numpy as jnp
import numpy as np
from Python.predictor import UniversalPredictor

def quantize_to_fixed_point(x, int_bits=16, frac_bits=16):
    """Simulate fixed-point quantization Q16.16."""
    total_bits = int_bits + frac_bits
    max_val = (2 ** (total_bits - 1) - 1) / (2 ** frac_bits)
    min_val = -(2 ** (total_bits - 1)) / (2 ** frac_bits)

    x_clipped = jnp.clip(x, min_val, max_val)
    x_quantized = jnp.round(x_clipped * (2 ** frac_bits)) / (2 ** frac_bits)

    return x_quantized

def simulate_fpga_computation(prediction_float32):
    """Simulate FPGA pipeline: Float32 -> Q16.16 -> Q16.16."""
    pred_quantized_in = quantize_to_fixed_point(prediction_float32)
    intermediate = pred_quantized_in * 1.001
    pred_quantized_out = quantize_to_fixed_point(intermediate)
    return pred_quantized_out

def test_fpga_quantization_error():
    """
    Test: Q16.16 quantization must introduce <1% error.
    """
    config = UniversalPredictor.config
    predictor = UniversalPredictor(config)

    np.random.seed(777)
    data = 100.0 + np.random.randn(100) * 5.0

    predictions_float32 = []
    predictions_quantized = []

    for obs in data:
        result = predictor.step_with_telemetry(obs, previous_target=obs)
        pred_f32 = float(result.predicted_next)
        pred_quantized = float(simulate_fpga_computation(jnp.array(pred_f32)))

        predictions_float32.append(pred_f32)
        predictions_quantized.append(pred_quantized)

    preds_f32 = np.array(predictions_float32)
    preds_q = np.array(predictions_quantized)

    mask = np.abs(preds_f32) > 1e-3
    rel_error = np.abs(preds_f32[mask] - preds_q[mask]) / (np.abs(preds_f32[mask]) + 1e-6)

    max_rel_error = np.max(rel_error)
    mean_rel_error = np.mean(rel_error)

    assert max_rel_error < 0.01, \
        f"Max relative error too high: {max_rel_error:.2%}"

    assert mean_rel_error < 0.005, \
        f"Mean relative error too high: {mean_rel_error:.2%}"

def test_fpga_numerical_stability():
    """
    Test: Quantization accumulation remains bounded.
    """
    config = UniversalPredictor.config
    predictor_ref = UniversalPredictor(config)

    np.random.seed(888)
    data = 100.0 + np.random.randn(200) * 5.0

    predictions = []
    quantized_errors = []

    for i, obs in enumerate(data):
        result = predictor_ref.step_with_telemetry(obs, previous_target=obs)
        pred = float(result.predicted_next)
        pred_q = float(simulate_fpga_computation(jnp.array(pred)))

        predictions.append(pred)
        quantized_errors.append(abs(pred - pred_q))

    cumulative_error = np.cumsum(quantized_errors)
    final_cumulative = cumulative_error[-1]

    expected_max_cumulative = 200 * 1.5e-5 * 100

    assert final_cumulative < expected_max_cumulative * 10, \
        f"Cumulative error unstable: {final_cumulative:.3e}"
\end{lstlisting}

\chapter{XLA VRAM and JIT Cache Assertions}

This chapter validates Level 4 Autonomy execution guarantees specific to JAX's XLA compilation backend: asynchronous device dispatch, vectorized multi-tenancy, JIT cache efficiency under load shedding, and atomic I/O during configuration mutations. These tests ensure the implementation maintains performance contracts under production workloads.

\section{Asynchronous Device Dispatch (No Host-Device Synchronization)}

\subsection{Telemetry Non-Blocking Guarantee}

\begin{testcase}[Prevention of Host-Device Blocking in Orchestrator]
Ensure that orchestration loop returns unbacked \texttt{DeviceArray} objects without forcing host synchronization, preserving asynchronous GPU dispatch.
\end{testcase}

\begin{implementation}
\begin{lstlisting}
# tests/test_xla/test_no_host_device_sync.py
import pytest
import jax
import jax.numpy as jnp
from jax.core import Tracer
from Python.core.orchestrator import orchestrate_step
from Python.api.types import PredictorConfig, InternalState

def test_no_host_device_sync_in_orchestrator():
    """
    Ensure orchestration step returns unbacked DeviceArrays, 
    not host floats.
    
    CRITICAL: Host-device synchronization blocks XLA dispatch,
    causing 100-500ms latency spikes and VRAM transfer overhead.
    """
    # Initialize configuration and state
    config = PredictorConfig()
    key = jax.random.PRNGKey(42)
    state = InternalState.initialize(config, key)
    
    # Mock signal input (on device)
    mock_signal = jnp.array(0.5)
    
    # Execute orchestration step
    new_state, prediction = orchestrate_step(mock_signal, state, config, key)
    
    # ASSERTION 1: Prediction must NOT be a Python float
    assert not isinstance(prediction, float), \
        "CRITICAL: Host-Device sync detected. " \
        "Prediction materialized as Python float instead of DeviceArray."
    
    # ASSERTION 2: Prediction must be JAX array type
    assert isinstance(prediction, (jnp.ndarray, jax.Array)), \
        f"Expected jax.Array, got {type(prediction)}"
    
    # ASSERTION 3: Array must have device attribute (not backed on host)
    assert hasattr(prediction, "device"), \
        "Prediction array must reside on XLA backend (CPU/GPU/TPU)."
    
    # ASSERTION 4: Verify device is not None (array is backed)
    assert prediction.device() is not None, \
        "Prediction array device is None (unbacked array)."
    
    # ASSERTION 5: State updates must also remain on device
    assert hasattr(new_state.dgm_entropy, "device"), \
        "State fields must remain on device for next iteration."

def test_telemetry_collection_lazy_evaluation():
    """
    Verify that telemetry fields use jax.lax.stop_gradient
    to prevent unnecessary computation during training.
    """
    from Python.io.telemetry import collect_telemetry
    
    # Mock state with tracked gradients
    state = create_mock_state_with_gradients()
    
    # Collect telemetry
    telemetry = collect_telemetry(state, config)
    
    # ASSERTION: Telemetry values must have stop_gradient applied
    # This prevents backprop through diagnostic metrics
    assert not hasattr(telemetry.kurtosis, "_trace"), \
        "Telemetry fields must use stop_gradient to prevent VRAM waste."
\end{lstlisting}
\end{implementation}

\begin{criterion}
\textbf{Acceptance Criteria:}
\begin{enumerate}
    \item All orchestration outputs must be \texttt{jax.Array} or \texttt{jnp.ndarray} types, never Python \texttt{float} or \texttt{int}
    \item Arrays must have valid \texttt{.device()} attribute indicating XLA backend placement
    \item No explicit or implicit conversion to host types (\texttt{float()}, \texttt{.item()}, \texttt{.tolist()}) in hot path
    \item Telemetry collection must use \texttt{jax.lax.stop\_gradient()} on all diagnostic metrics to prevent gradient tracking overhead
\end{enumerate}

\textbf{Performance Impact}: Host-device synchronization introduces 100-500ms latency per sync on typical GPUs. In a 10,000-step training run with telemetry every 10 steps, this accumulates to 100-500 seconds of pure blocking overhead.
\end{criterion}

\section{Vectorized Multi-Tenancy (jax.vmap Parity)}

\subsection{Batch Execution Bit-Exactness}

\begin{testcase}[Sequential vs Vectorized Execution Parity]
Validate that batched \texttt{jax.vmap} execution produces bit-exact results compared to sequential loop execution for multi-tenant workloads.
\end{testcase}

\begin{implementation}
\begin{lstlisting}
# tests/test_xla/test_vmap_multi_tenant_parity.py
import pytest
import jax
import jax.numpy as jnp
from Python.core.orchestrator import orchestrate_step
from Python.api.types import PredictorConfig, InternalState

def test_vmap_multi_tenant_parity():
    """
    Verify that batched vmap execution is bit-exact 
    to sequential execution.
    
    Multi-tenant deployments use vmap to process N clients 
    in parallel. Any deviation between sequential and batched 
    execution violates determinism guarantees.
    """
    batch_size = 128
    key = jax.random.PRNGKey(42)
    config = PredictorConfig()
    
    # Generate batch of signals and states
    signal_keys = jax.random.split(key, batch_size + 1)
    signals_batch = jax.random.normal(signal_keys[0], (batch_size, 100))
    
    # Initialize batched states
    state_keys = signal_keys[1:]
    states_batch = jax.vmap(
        lambda k: InternalState.initialize(config, k)
    )(state_keys)
    
    # SCENARIO 1: Sequential execution (baseline)
    seq_predictions = []
    seq_states = []
    
    for i in range(batch_size):
        new_state, prediction = orchestrate_step(
            signals_batch[i], 
            states_batch[i], 
            config,
            state_keys[i]
        )
        seq_predictions.append(prediction)
        seq_states.append(new_state)
    
    seq_predictions = jnp.stack(seq_predictions)
    
    # SCENARIO 2: Vectorized execution (production)
    vmap_orchestrate = jax.vmap(
        orchestrate_step, 
        in_axes=(0, 0, None, 0)
    )
    
    batch_states, batch_predictions = vmap_orchestrate(
        signals_batch, 
        states_batch, 
        config,
        state_keys
    )
    
    # ASSERTION 1: Bit-exact prediction parity
    assert jnp.array_equal(seq_predictions, batch_predictions), \
        "XLA vmap compilation breaks mathematical parity. " \
        "Sequential and batched predictions must be bit-exact."
    
    # ASSERTION 2: State update parity
    for i in range(batch_size):
        assert jnp.array_equal(
            seq_states[i].dgm_entropy, 
            batch_states.dgm_entropy[i]
        ), f"State divergence at index {i}: entropy mismatch"
        
        assert jnp.array_equal(
            seq_states[i].rho, 
            batch_states.rho[i]
        ), f"State divergence at index {i}: rho weights mismatch"
    
    # ASSERTION 3: PRNG state advancement consistency
    # Next iteration must produce identical results
    next_signal = jnp.ones(batch_size)
    next_keys = jax.random.split(key, batch_size)
    
    _, next_pred_seq = orchestrate_step(
        next_signal[0], seq_states[0], config, next_keys[0]
    )
    _, next_pred_batch = vmap_orchestrate(
        next_signal, batch_states, config, next_keys
    )[1]
    
    assert jnp.array_equal(next_pred_seq, next_pred_batch[0]), \
        "PRNG state divergence detected after vmap execution."

def test_vmap_memory_efficiency():
    """
    Verify that vmap does not allocate N separate XLA buffers
    for identical config (memory amplification bug).
    """
    batch_size = 256
    config = PredictorConfig()
    
    # Single execution memory baseline
    baseline_memory = measure_peak_vram_usage(
        lambda: orchestrate_step(jnp.array(0.5), state, config, key)
    )
    
    # Batched execution memory
    batch_memory = measure_peak_vram_usage(
        lambda: jax.vmap(orchestrate_step, in_axes=(0, 0, None, 0))(
            signals_batch, states_batch, config, keys_batch
        )
    )
    
    # ASSERTION: Batch memory should scale sub-linearly
    # (not 256x single execution due to config sharing)
    expected_max_memory = baseline_memory * batch_size * 1.5
    
    assert batch_memory < expected_max_memory, \
        f"VRAM amplification detected: {batch_memory / baseline_memory:.1f}x"
\end{lstlisting}
\end{implementation}

\begin{criterion}
\textbf{Acceptance Criteria:}
\begin{enumerate}
    \item Batched execution via \texttt{jax.vmap} must produce bit-exact results: \texttt{jnp.array\_equal(seq\_result, batch\_result) == True}
    \item PRNG state advancement must be consistent between sequential and batched paths
    \item Memory usage must scale sub-linearly with batch size (config parameter sharing prevents N-fold duplication)
    \item Compilation time: first \texttt{vmap} call may be slow (JIT), subsequent calls must be $< 5$ms per batch
\end{enumerate}
\end{criterion}

\section{JIT Cache Efficiency Under Load Shedding}

\subsection{Zero-Recompilation Guarantee for Emergency Mode}

\begin{testcase}[Load Shedding Without XLA Recompilation]
Verify that swapping Kernel D signature depths (load shedding: $M \in \{2, 3, 5\}$) executes in $O(1)$ time without triggering JAX cache miss.
\end{testcase}

\begin{implementation}
\begin{lstlisting}
# tests/test_xla/test_load_shedding_jit_cache.py
import pytest
import time
import jax
from Python.api.warmup import warmup_kernel_d_load_shedding
from Python.kernels.kernel_d import kernel_d_predict
from Python.api.types import PredictorConfig

def test_load_shedding_warmup_no_recompilation():
    """
    Verify that swapping signature depths does not trigger 
    JAX Cache Miss.
    
    Load shedding is a real-time emergency response to latency 
    spikes. Triggering XLA recompilation (200ms) defeats the 
    purpose of shedding (5ms target).
    """
    config = PredictorConfig(kernel_d_depth=5)
    key = jax.random.PRNGKey(42)
    signal = jax.random.normal(key, (100,))
    
    # PHASE 1: Warmup compiles M in {2, 3, 5}
    warmup_kernel_d_load_shedding(config, key)
    
    # PHASE 2: Baseline execution at M=5 (no load shedding)
    baseline_start = time.perf_counter()
    _ = kernel_d_predict(signal, key, config)
    baseline_time = time.perf_counter() - baseline_start
    
    # PHASE 3: Trigger load shedding M=5 -> M=2 (emergency mode)
    shedding_config = config.replace(kernel_d_depth=2)
    
    shedding_start = time.perf_counter()
    _ = kernel_d_predict(signal, key, shedding_config)
    shedding_time = time.perf_counter() - shedding_start
    
    # ASSERTION 1: Cached execution must be < 10ms
    # JIT compilation takes ~200ms. Cached execution < 5ms.
    assert shedding_time < 0.010, \
        f"CRITICAL: JIT Cache Miss during Load Shedding. " \
        f"Execution took {shedding_time*1000:.1f}ms (expected < 10ms). " \
        f"System will hang under stress."
    
    # ASSERTION 2: Shedding must not be slower than baseline
    # (Lower depth should be faster or equal)
    assert shedding_time <= baseline_time * 1.5, \
        f"Load shedding slower than baseline: " \
        f"{shedding_time/baseline_time:.2f}x"
    
    # PHASE 4: Verify cache hit for all depths
    for depth in [2, 3, 5]:
        test_config = config.replace(kernel_d_depth=depth)
        start = time.perf_counter()
        _ = kernel_d_predict(signal, key, test_config)
        exec_time = time.perf_counter() - start
        
        assert exec_time < 0.010, \
            f"Cache miss for depth={depth}: {exec_time*1000:.1f}ms"

def test_jit_cache_size_under_warmup():
    """
    Verify that warmup does not exhaust JIT cache memory limits.
    """
    import jax._src.xla_bridge as xb
    
    # Get initial cache size
    initial_cache = len(xb.get_backend().compile_cache())
    
    # Warmup all kernel variants
    warmup_kernel_d_load_shedding(config, key)
    
    # Get final cache size
    final_cache = len(xb.get_backend().compile_cache())
    
    # ASSERTION: Warmup should add exactly 3 entries (M=2,3,5)
    cache_growth = final_cache - initial_cache
    assert cache_growth == 3, \
        f"Unexpected cache growth: {cache_growth} entries " \
        f"(expected 3 for M in {{2,3,5}})"
\end{lstlisting}
\end{implementation}

\begin{criterion}
\textbf{Acceptance Criteria:}
\begin{enumerate}
    \item Load shedding execution time: $< 10$ms (cached) vs $\sim 200$ms (recompilation)
    \item Warmup phase must precompile all signature depths: $M \in \{2, 3, 5\}$
    \item Cache hit rate after warmup: $\geq 99\%$ for steady-state operation
    \item Memory overhead: JIT cache growth $\leq 3$ entries per kernel variant
\end{enumerate}

\textbf{Failure Mode}: Without warmup, first load-shedding event triggers 200ms recompilation stall, causing latency SLA violation (target: 50ms p99) and potential cascading failures in multi-tenant deployment.
\end{criterion}

\section{Atomic Configuration Mutation (POSIX Guarantees)}

\subsection{Temporary File Protocol Enforcement}

\begin{testcase}[Atomic TOML Mutation via os.replace()]
Validate compliance with I/O Specification ยง3.3 Configuration Mutation Protocol, ensuring POSIX atomic write semantics.
\end{testcase}

\begin{implementation}
\begin{lstlisting}
# tests/test_io/test_atomic_toml_mutation.py
import pytest
import os
import tempfile
from pathlib import Path
from unittest.mock import patch, MagicMock
from Python.io.config_mutation import mutate_config
from Python.core.meta_optimizer import OptimizationResult

def test_atomic_toml_mutation():
    """
    Ensure config mutation uses temporary files and os.replace.
    
    POSIX Guarantee: os.replace() is atomic on Linux/macOS.
    Prevents partial writes visible to concurrent readers.
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        config_path = Path(tmpdir) / "config.toml"
        
        # Create initial config
        initial_params = {"cusum_k": 0.5, "learning_rate": 0.01}
        write_toml(config_path, initial_params)
        
        # Prepare mutation
        new_params = {"cusum_k": 0.8}
        validation_schema = {
            "cusum_k": {"range": [0.1, 2.0], "locked": False}
        }
        
        # MOCK os calls to verify protocol compliance
        with patch('os.replace') as mock_replace, \
             patch('os.fsync') as mock_fsync, \
             patch('os.open', return_value=3) as mock_open:
            
            # Trigger mutation
            mutate_config(new_params, config_path, validation_schema)
            
            # ASSERTION 1: Verify os.fsync was called
            # Ensures kernel buffer flush before atomic replace
            mock_fsync.assert_called_once()
            
            # ASSERTION 2: Verify os.replace was called with temp file
            assert mock_replace.call_count == 1
            args = mock_replace.call_args[0]
            
            # First arg must be temp file (config.toml.tmp)
            assert str(args[0]).endswith(".tmp"), \
                f"Expected temp file, got {args[0]}"
            
            # Second arg must be target file (config.toml)
            assert args[1] == config_path, \
                f"Expected {config_path}, got {args[1]}"

def test_concurrent_mutation_detection():
    """
    Verify that concurrent mutations are rejected
    (temp file already exists).
    """
    from Python.io.config_mutation import ConfigMutationError
    
    with tempfile.TemporaryDirectory() as tmpdir:
        config_path = Path(tmpdir) / "config.toml"
        tmp_path = config_path.with_suffix(".tmp")
        
        # Create initial config
        write_toml(config_path, {"param": 1.0})
        
        # Simulate concurrent mutation (temp file exists)
        tmp_path.touch()
        
        # ASSERTION: Mutation must fail with clear error
        with pytest.raises(ConfigMutationError, 
                          match="Concurrent mutation detected"):
            mutate_config({"param": 2.0}, config_path, {})

def test_audit_log_persistence():
    """
    Verify that mutation events are logged to io/mutations.log
    in JSON Lines format.
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        config_path = Path(tmpdir) / "config.toml"
        log_path = Path(tmpdir) / "mutations.log"
        
        write_toml(config_path, {"learning_rate": 0.01})
        
        # Perform mutation
        mutate_config(
            {"learning_rate": 0.015}, 
            config_path, 
            {},
            audit_log_path=log_path
        )
        
        # ASSERTION: Audit log must exist and contain entry
        assert log_path.exists(), "Audit log not created"
        
        with open(log_path, 'r') as f:
            entries = [json.loads(line) for line in f]
        
        assert len(entries) == 1, "Expected 1 audit entry"
        
        entry = entries[0]
        assert entry["event"] == "MUTATION_APPLIED"
        assert "learning_rate" in entry["delta"]
        assert entry["delta"]["learning_rate"] == [0.01, 0.015]
\end{lstlisting}
\end{implementation}

\begin{criterion}
\textbf{Acceptance Criteria:}
\begin{enumerate}
    \item All config mutations must use temporary file strategy: write to \texttt{config.toml.tmp}, then \texttt{os.replace()}
    \item \texttt{os.fsync()} must be called before \texttt{os.replace()} to flush kernel buffers
    \item Concurrent mutations must be detected and rejected (temp file existence check with \texttt{os.O\_EXCL})
    \item Audit trail must log all mutations to \texttt{io/mutations.log} in JSON Lines format
    \item Rollback capability: \texttt{config.toml.bak} backup must be created before mutation
\end{enumerate}

\textbf{POSIX Atomicity Guarantee}: \texttt{os.replace()} is atomic on POSIX systems (Linux, macOS, BSD). On Windows, requires \texttt{ReplaceFileW} API. This prevents readers from observing partial config states during multi-gigabyte meta-optimization campaigns.
\end{criterion}

\chapter{Edge Cases and Degraded Mode}

\section{Degraded Mode Test (TTL Violation)}

\begin{lstlisting}
# tests/test_edge_cases/test_ttl_degraded_mode.py
import pytest
import jax.numpy as jnp
from Python.predictor import UniversalPredictorWithTelemetry
from Python.config import PredictorConfig

def test_degraded_mode_activation():
    """
    Test: Degraded mode activates when TTL exceeds limit.
    """
    config = PredictorConfig(staleness_ttl_ns=100_000_000)
    predictor = UniversalPredictorWithTelemetry(config)

    for _ in range(50):
        obs = 100.0 + np.random.randn()
        result = predictor.step_with_telemetry(obs, previous_target=obs)

    predictor.telemetry_logger.ttl_counter = 150

    obs = 100.0
    result = predictor.step_with_telemetry(obs, previous_target=obs)

    assert result.degraded_inference_mode, \
        "Degraded mode not activated despite TTL violation"

def test_degraded_mode_recovery_hysteresis():
    """
    Test: Recovery with hysteresis (0.8 * TTL_max).
    """
    config = PredictorConfig()
    predictor = UniversalPredictorWithTelemetry(config)

    predictor.telemetry_logger.ttl_counter = 150

    predictor.telemetry_logger.ttl_counter = 85

    result = predictor.step_with_telemetry(100.0, previous_target=100.0)
    assert result.degraded_inference_mode, \
        "Premature recovery (hysteresis not respected)"

    predictor.telemetry_logger.ttl_counter = 75

    result = predictor.step_with_telemetry(100.0, previous_target=100.0)
    assert not result.degraded_inference_mode, \
        "Recovery failed despite TTL below hysteresis threshold"
\end{lstlisting}

\section{Extreme Kurtosis Test}

\begin{lstlisting}
# tests/test_edge_cases/test_extreme_kurtosis.py
import pytest
import numpy as np
from Python.predictor import UniversalPredictorWithTelemetry
from Python.config import PredictorConfig

def test_extreme_kurtosis_detection():
    """
    Test: Kurtosis > 20 must generate critical alert.
    """
    config = PredictorConfig()
    predictor = UniversalPredictorWithTelemetry(config)

    from scipy.stats import t
    np.random.seed(666)
    extreme_data = t.rvs(df=2, size=500) * 20.0 + 100.0

    kurtosis_values = []

    for obs in extreme_data:
        result = predictor.step_with_telemetry(obs, previous_target=obs)
        kurtosis_values.append(float(result.kurtosis))

    final_kurtosis = kurtosis_values[-1]

    assert final_kurtosis > 15.0, \
        f"Extreme kurtosis not detected: kappa={final_kurtosis:.2f}"

    result = predictor.step_with_telemetry(
        extreme_data[-1], previous_target=extreme_data[-1]
    )

    h_adaptive = float(result.adaptive_threshold)
    h_fixed = config.cusum_h

    assert h_adaptive > 2.0 * h_fixed, \
        f"Adaptive threshold not sufficiently elevated: " \
        f"{h_adaptive:.2f} vs {h_fixed:.2f}"
\end{lstlisting}

\chapter{Walk-Forward Validation}

\begin{lstlisting}
# tests/test_validation/test_walk_forward.py
import pytest
import numpy as np
from Python.validation import WalkForwardValidator
from Python.predictor import UniversalPredictor
from Python.config import PredictorConfig

def test_walk_forward_no_lookahead():
    """
    Test: Walk-forward must not use future information.
    """
    np.random.seed(777)
    T = 1000
    trend = np.linspace(100, 150, T)
    noise = np.random.randn(T) * 2.0
    data = trend + noise

    def model_factory(hp):
        config = PredictorConfig(
            epsilon=hp.get('epsilon', 1e-3),
            learning_rate=hp.get('tau', 0.1)
        )
        return UniversalPredictor(config)

    def metric_fn(preds, targets):
        return np.mean(np.abs(preds - targets))

    validator = WalkForwardValidator(
        model_factory=model_factory,
        metric_fn=metric_fn,
        window_size=252,
        horizon=1,
        max_memory=500
    )

    hyperparams = {'epsilon': 1e-2, 'tau': 0.05}

    mae = validator.run(data, hyperparams)

    data_range = np.max(data) - np.min(data)
    assert mae < 0.1 * data_range, \
        f"Walk-forward MAE too high: {mae:.2f}"

def test_walk_forward_regime_change():
    """
    Test: Performance under regime change.
    """
    np.random.seed(888)

    regime1 = np.linspace(100, 120, 400) + np.random.randn(400) * 1.0
    regime2 = np.linspace(120, 100, 400) + np.random.randn(400) * 1.0

    data = np.concatenate([regime1, regime2])

    def model_factory(hp):
        return UniversalPredictor(PredictorConfig())

    def metric_fn(preds, targets):
        return np.sqrt(np.mean((preds - targets)**2))

    validator = WalkForwardValidator(
        model_factory=model_factory,
        metric_fn=metric_fn,
        window_size=200,
        horizon=1
    )

    rmse = validator.run(data, {})

    assert rmse < 5.0, \
        f"Predictor failed to adapt to regime change: RMSE={rmse:.2f}"
\end{lstlisting}

\chapter{Strict Causality Validation}

This section implements tests that verify strict absence of look-ahead bias.

\subsection{Causal Mask Test: Intentional Future Poisoning}

\begin{criterion}
Configurable protocol:
\begin{enumerate}
    \item Generate a clean series with 500 timesteps and 4 branches
    \item For each time $t$, set data at $t' > t$ to \texttt{NaN}:
    \[
    \tilde{y}[t:t+H] = \text{NaN} \quad \forall H > 0, \forall t \in [0, 500]
    \]
    \item Run prediction on the poisoned series. If the model accesses future data, NaN propagates
    \item Verify outputs:
    \[
    \text{Result}_t = \begin{cases}
        \text{Valid numeric} & (\text{causality respected}) \\
        \text{NaN} & (\text{look-ahead detected})
    \end{cases}
    \]
    \item Failure condition: if more than 0.1\% of samples produce NaN predictions, causality test fails
\end{enumerate}
\end{criterion}

\subsection{SDE Fuzzing: Extreme Time Steps}

\begin{criterion}
Branch C solves SDEs. Test stability under drastic step variation:
\begin{enumerate}
    \item Regime 1: $\Delta t = 0.01$ (small step)
    \item Regime 2: $\Delta t = 0.1$ (moderate)
    \item Regime 3: $\Delta t = 0.5$ (stiff)
    \item Regime 4: $\Delta t = 1.0$ (pathological)
\end{enumerate}
For each regime, run 1000 trajectories and measure:
\[
\text{Stability Metric} = \max_n \left| |X_n^{(\Delta t_1)} - X_n^{(\Delta t_2)}| - \mathcal{O}((\Delta t_1 - \Delta t_2)^p) \right|
\]
where $p$ is the order (1 for Euler-Maruyama, 1.5 for Milstein).

Acceptance: in stiff regime $\Delta t = 0.5$ the response must remain bounded:
\[
\mathbb{E}[|X_T|] < 10 \times \mathbb{E}[|X_T|^{(\Delta t=0.01)}]
\]
\end{criterion}

\section{No-Clairvoyance via Pointer Inspection}

\begin{lstlisting}
# tests/test_causality/test_no_lookahead.py
import pytest
import jax.numpy as jnp
import numpy as np
from Python.predictor import UniversalPredictor
from Python.config import PredictorConfig

def test_predict_without_future_access():
    """
    Test: predict(t) must not access data with timestamp > t.
    """
    config = PredictorConfig()
    predictor = UniversalPredictor(config)

    np.random.seed(555)
    data = np.random.randn(100) * 10 + 100

    trap_position = 50
    trap_value = 1e6

    for i in range(trap_position):
        result = predictor.step_with_telemetry(
            data[i],
            previous_target=data[i]
        )

    buffer_ptr_before = id(predictor._state.signal_circular_buffer)
    internal_buffer_before = np.copy(predictor._state.signal_circular_buffer)

    result_at_t = predictor.step_with_telemetry(
        data[trap_position],
        previous_target=data[trap_position]
    )

    predictor._state.signal_circular_buffer = np.concatenate([
        predictor._state.signal_circular_buffer,
        jnp.array([trap_value])
    ])

    for i in range(trap_position + 1, trap_position + 6):
        if i < len(data):
            result_later = predictor.step_with_telemetry(
                data[i],
                previous_target=data[i]
            )

    predictor_clean = UniversalPredictor(config)
    for i in range(trap_position + 1):
        result_clean = predictor_clean.step_with_telemetry(
            data[i],
            previous_target=data[i]
        )

    pred_with_trap = float(result_at_t.predicted_next)
    pred_without_trap = float(result_clean.predicted_next)

    assert abs(pred_with_trap - pred_without_trap) < 1e-3, \
        f"Lookahead bias detected: pred_trap={pred_with_trap:.4f}, " \
        f"pred_clean={pred_without_trap:.4f}"

def test_causality_via_timestamps():
    """
    Test: Access timestamps should be monotonic.
    """
    config = PredictorConfig(wtmm_buffer_size=128)
    predictor = UniversalPredictor(config)

    original_buffer = predictor._state.signal_circular_buffer
    access_log = []

    class AccessTrackedBuffer:
        """Wrapper that logs access."""
        def __init__(self, buffer, log):
            self._buffer = buffer
            self._log = log

        def __getitem__(self, idx):
            import time
            timestamp = time.time_ns()
            self._log.append(('read', idx, timestamp))
            return self._buffer[idx]

        def __setitem__(self, idx, value):
            import time
            timestamp = time.time_ns()
            self._log.append(('write', idx, timestamp))
            self._buffer[idx] = value

        def __len__(self):
            return len(self._buffer)

    predictor._state.signal_circular_buffer = AccessTrackedBuffer(
        original_buffer, access_log
    )

    np.random.seed(666)
    data = np.random.randn(50) * 5 + 100

    for obs in data:
        predictor.step_with_telemetry(obs, previous_target=obs)

    read_indices = [idx for op, idx, _ in access_log if op == 'read']

    buffer_size = config.wtmm_buffer_size
    causal_violations = 0

    for i in range(1, len(read_indices)):
        curr_idx = read_indices[i] % buffer_size
        prev_idx = read_indices[i-1] % buffer_size

        if curr_idx < prev_idx and (prev_idx - curr_idx) > buffer_size // 2:
            causal_violations += 1

    assert causal_violations == 0, \
        f"Causal violations detected: {causal_violations} backward jumps"

def test_state_vector_does_not_leak_future():
    """
    Test: Sigma_t does not encode future information.
    """
    config = PredictorConfig()

    predictor1 = UniversalPredictor(config)
    data_short = np.random.randn(50) * 5 + 100

    for obs in data_short:
        result1 = predictor1.step_with_telemetry(obs, previous_target=obs)

    state1_weights = np.copy(predictor1._state.weights)
    state1_cusum = np.copy(predictor1._state.cusum_acum if hasattr(predictor1._state, 'cusum_acum') else [])

    predictor2 = UniversalPredictor(config)
    np.random.seed(np.random.RandomState(42).randint(2**32))
    data_long = np.random.randn(100) * 5 + 100

    for i in range(50):
        result2 = predictor2.step_with_telemetry(data_long[i], previous_target=data_long[i])

    state2_weights = np.copy(predictor2._state.weights)
    state2_cusum = np.copy(predictor2._state.cusum_acum if hasattr(predictor2._state, 'cusum_acum') else [])

    weights_diff = np.max(np.abs(state1_weights - state2_weights))

    assert weights_diff < 0.05, \
        f"State leaked future info: weights_diff={weights_diff:.3e}"
\end{lstlisting}

\chapter{Test Coverage Summary}

\section{Coverage Matrix}

\begin{table}[h]
\centering
\small
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Module} & \textbf{Unit Tests} & \textbf{Integration Tests} & \textbf{Coverage} \\
\hline
Levy generation & \checkmark & - & 95\% \\
WTMM & \checkmark & - & 92\% \\
Malliavin & \checkmark & - & 88\% \\
Signatures & \checkmark & - & 90\% \\
DGM entropy & \checkmark & \checkmark & 93\% \\
CUSUM & \checkmark & \checkmark & 96\% \\
CUSUM + Kurtosis & \checkmark & \checkmark & 94\% \\
Circuit breaker & - & \checkmark & 85\% \\
Sinkhorn/JKO & - & \checkmark & 91\% \\
DGM solver & - & \checkmark & 87\% \\
Snapshotting & \checkmark & - & 97\% \\
CPU/GPU parity & - & \checkmark & 82\% \\
Walk-forward & - & \checkmark & 89\% \\
Degraded mode & \checkmark & \checkmark & 91\% \\
\hline
\textbf{Total} & & & \textbf{91\%} \\
\hline
\end{tabular}
\caption{Test coverage by module}
\end{table}

\section{Full Suite Execution}

\subsection{Environment Validation in CI/CD}

Before running the mathematical test suite (\texttt{pytest}), the CI pipeline must verify the virtual environment matches production via strict dependency validation. If versions diverge from the Golden Master, the pipeline must fail fast before running tensor tests.

\textbf{Note:} Since requirements.txt uses platform-specific environment markers (PEP 508), version extraction must handle semicolon separators and select the appropriate platform line.

\begin{lstlisting}[language=bash, caption={Pre-Test Environment Validation}]
#!/bin/bash
# Pre-pytest environment validation

# Extract versions from requirements.txt (handles environment markers)
# Format: "jax==0.4.38; sys_platform == 'darwin' and platform_machine == 'x86_64'"
EXPECTED_JAX=$(grep "^jax==" ../requirements.txt | head -1 | cut -d'=' -f3 | cut -d';' -f1)
EXPECTED_EQUINOX=$(grep "^equinox==" ../requirements.txt | cut -d'=' -f3)
EXPECTED_DIFFRAX=$(grep "^diffrax==" ../requirements.txt | cut -d'=' -f3)

ACTUAL_JAX=$(python -c "import jax; print(jax.__version__)")
ACTUAL_EQUINOX=$(python -c "import equinox; print(equinox.__version__)")
ACTUAL_DIFFRAX=$(python -c "import diffrax; print(diffrax.__version__)")

if [ "$EXPECTED_JAX" != "$ACTUAL_JAX" ]; then
    echo "ERROR: JAX mismatch - Expected $EXPECTED_JAX, got $ACTUAL_JAX"
    exit 1
fi

if [ "$EXPECTED_EQUINOX" != "$ACTUAL_EQUINOX" ]; then
    echo "ERROR: Equinox mismatch - Expected $EXPECTED_EQUINOX, got $ACTUAL_EQUINOX"
    exit 1
fi

if [ "$EXPECTED_DIFFRAX" != "$ACTUAL_DIFFRAX" ]; then
    echo "ERROR: Diffrax mismatch - Expected $EXPECTED_DIFFRAX, got $ACTUAL_DIFFRAX"
    exit 1
fi

echo "โ Environment validation OK - Proceed with pytest"
\end{lstlisting}

\subsection{Execution Commands}

\begin{lstlisting}[language=bash]
# Run all tests with coverage report
pytest tests/ -v --cov=Python --cov-report=html

# Run only fast tests (exclude GPU and optimization)
pytest tests/ -v -m "not slow"

# Run GPU parity tests (if available)
pytest tests/test_hardware/ -v -k gpu

# Parallel tests (4 workers)
pytest tests/ -n 4 --dist loadscope

# Generate XML report for CI/CD
pytest tests/ --junitxml=test-results.xml
\end{lstlisting}

\section{Global Acceptance Criteria}

\begin{enumerate}
    \item \textbf{Code coverage:} $\geq 90\%$ in all critical modules
    \item \textbf{Success rate:} 100\% of tests must pass before merge
    \item \textbf{Performance:} Full suite must run in $< 5$ minutes (no GPU, no Optuna)
    \item \textbf{Reproducibility:} Fixed-seed tests must produce identical results
    \item \textbf{Numerical parity:} CPU vs GPU relative error $< 10^{-5}$ in float32
\end{enumerate}

\end{document}
