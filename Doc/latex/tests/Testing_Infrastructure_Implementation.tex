\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}
\usepackage{xurl}

% Custom hyperlink commands for file and document references
\newcommand{\filehref}[1]{\href{file:../../#1}{\texttt{#1}}}
\newcommand{\dochref}[2]{\href{../../pdf/specification/#1.pdf}{\texttt{#2}}}

\geometry{margin=1in}
\pagestyle{fancy}
\fancyhf{}
\lhead{USP Testing Infrastructure Implementation}
\rhead{v2.1.0 | 2026-02-21}
\cfoot{\thepage}

% Code listings
\lstset{
    language=bash,
    basicstyle=\ttfamily\small,
    breaklines=true,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    backgroundcolor=\color{lightgray!20},
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

\title{\textbf{Universal Stochastic Predictor}\\[0.5em]
       \textbf{Testing Infrastructure Implementation}\\[0.5em]
       \large Complete Deployment, Usage Guide, and Expected Outcomes}
\author{Development Team}
\date{Document Version: 2.0 \\ Last Updated: 2026-02-21 \\ Test Framework Implementation: v2.1.0}

\begin{document}

\maketitle
\thispagestyle{fancy}

\begin{abstract}
\noindent This is a comprehensive \textbf{Implementation Document} describing the complete deployment, operational usage, and expected outcomes of the Universal Stochastic Predictor's automated testing infrastructure. It documents how the system has been architected and deployed, how practitioners use it in daily development workflows, and what measurable quality and performance outcomes to expect.

\noindent\textbf{Scope of this Document:}
\begin{enumerate}[noitemsep,leftmargin=1.5em]
    \item \textbf{Deployment Strategy:} How the three-phase system was constructed and integrated
    \item \textbf{Usage Patterns:} How developers and CI/CD systems invoke the testing framework
    \item \textbf{Expected Outcomes:} Quantified quality metrics, performance benchmarks, and success criteria
\end{enumerate}

\noindent The system features dynamic module discovery without hardcoding, intelligent change detection for incremental testing (13--50$\times$ speedup in typical compliance workflows with \texttt{code\_alignement.py}), comprehensive JSON and Markdown reporting, and extensible architecture for adding new validation rules and tests.
\end{abstract}

\tableofcontents
\newpage

% ============================================================================
\section{System Overview}
% ============================================================================

\subsection{Objectives}

The testing infrastructure serves four primary objectives:

\begin{enumerate}
    \item \textbf{Code Quality Validation:} Enforce PEP 8 standards, type safety, and styling rules via \texttt{flake8}, \texttt{mypy}, \texttt{black}, and \texttt{isort}.
    
    \item \textbf{Policy Compliance Auditing:} Verify all 36 architecture policies are satisfied (see \dochref{../tests/Code_Testing_Audit_Policies}{Code\_Testing\_Audit\_Policies.tex}).
    
    \item \textbf{Functional Correctness:} Execute comprehensive unit tests targeting all API, Core, Kernel, and I/O modules.
    
    \item \textbf{Performance Optimization:} Detect only modified files since last run, reducing \texttt{code\_alignement.py} execution time from 12.5s to $<$1s in typical workflows (incremental mode).
\end{enumerate}

\subsection{Architectural Layers}

The testing system is organized into five layers (matching the system architecture):

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Layer} & \textbf{Components} & \textbf{Tests} \\
\hline
API & \texttt{config.py}, \texttt{prng.py}, \texttt{schemas.py}, etc. & Warmup, validation \\
Core & \texttt{orchestrator.py}, \texttt{fusion.py}, etc. & Orchestration, meta-opt \\
Kernels & \texttt{kernel\_a/b/c/d.py}, \texttt{base.py} & Prediction, singularities \\
I/O & \texttt{snapshots.py}, \texttt{telemetry.py}, etc. & Persistence, telemetry \\
Tests & \texttt{code\_lint.py}, \texttt{scope\_discovery.py}, etc. & This infrastructure \\
\hline
\end{tabular}
\end{center}

\subsection{Pipeline Execution}

The default test pipeline executes in strict sequence with fail-fast semantics:

\begin{enumerate}
    \item[0.] \textbf{Dependency Check:} \filehref{Test/scripts/dependency\_check.py} validates Golden Master compliance (always runs first)
    \item \textbf{Linting Phase:} \filehref{Test/scripts/code\_lint.py} validates style and formatting
    \item \textbf{Compliance Phase:} \filehref{Test/scripts/code\_alignement.py} audits 36 policies (with cache optimization)
    \item \textbf{Execution Phase:} \filehref{Test/scripts/code\_structure.py} runs 79 unit tests with real JAX execution
\end{enumerate}

If any phase fails, subsequent phases are skipped and the pipeline exits with code 1.

\textbf{Critical:} Dependency version check runs before all other tests to ensure environment matches \texttt{requirements.txt}. This prevents false test failures caused by version mismatches.

% ============================================================================
\section{Deployment Strategy}
% ============================================================================

\subsection{Development Timeline}

The testing infrastructure was developed incrementally to address specific project needs:

\begin{enumerate}
    \item \textbf{Phase 1 (Initial):} \texttt{code\_alignement.py} implemented to enforce 38 architecture policies from specification
    \item \textbf{Phase 2:} \texttt{code\_structure.py} added for comprehensive unit testing with pytest + JAX integration
    \item \textbf{Phase 3:} \texttt{scope\_discovery.py} introduced to eliminate hardcoded module lists and enable automatic adaptation
    \item \textbf{Phase 4:} \texttt{code\_lint.py} and change detection system added for workflow optimization
    \item \textbf{Phase 5 (Current):} \texttt{dependency\_check.py} added for Golden Master validation with platform-aware environment marker parsing
\end{enumerate}

\subsection{System Architecture Decisions}

\subsubsection{Why Four-Phase Pipeline?}

The pipeline is structured as four independent validation stages rather than a monolithic script:

\begin{center}
\begin{tabular}{|l|p{6cm}|l|}
\hline
\textbf{Stage} & \textbf{Rationale} & \textbf{Why Separate?} \\
\hline
Dependency Check & Validates installed package versions against Golden Master & Fail-fast on env mismatch \\
\hline
Linting & Fast preliminary checks (milliseconds per file) & Fail early on style issues \\
\hline
Compliance & Policy validation across repository. Slow but essential. & Cannot be parallelized \\
\hline
Execution & Resource-intensive functional tests with JAX & Only run if earlier stages pass \\
\hline
\end{tabular}
\end{center}

\textbf{Benefit:} Fail-fast semantics allow developers to fix issues progressively without running expensive tests.

\subsubsection{Why Auto-Discovery Instead of Hardcoding?}

Original design used explicit module lists:

\begin{lstlisting}[language=python]
# Old approach (brittle)
MODULES = ['api', 'core', 'kernels', 'io']  # Must update manually
\end{lstlisting}

Problems with hardcoding:
\begin{itemize}[noitemsep]
    \item Adding new module requires editing 3+ scripts
    \item Risk: New modules accidentally omitted from tests
    \item Maintenance burden: Changes propagate poorly
\end{itemize}

Current approach uses filesystem discovery:

\begin{lstlisting}[language=python]
# New approach (automatic)
def discover_modules(root):
    python_dir = root / "Python"
    init_files = sorted(python_dir.glob("*/__init__.py"))
    return [init_file.parent.name for init_file in init_files]
# Finds: ['api', 'core', 'kernels', 'io'] automatically
\end{lstlisting}

\textbf{Result:} Add new module to \texttt{Python/}, tests discover it automatically. Zero configuration.

\subsubsection{Why Change Detection?}

During development, running 79 tests on every save is unproductive. Analysis:

\begin{itemize}[noitemsep]
    \item Typical commit: 2-5 files modified
    \item Full test run: 45 seconds
    \item Incremental run: 3-5 seconds
    \item \textbf{Productivity impact:} Developers don't wait, tests get skipped
\end{itemize}

Solution: Intelligent caching of file modification times.

\begin{lstlisting}[language=python]
# Store modification times in cache
{
  "Python/api/config.py": 1708456789.123,
  "Python/core/orchestrator.py": 1708456800.456,
  ...
}

# On next run: only test files with changed mtimes
\end{lstlisting}

\textbf{Trade-off:} Requires cache reset if external tools modify files, but \texttt{--force} and \texttt{--reset-cache} flags always available.

\subsection{Integration with Version Control}

The testing infrastructure assumes git-based development workflow:

\begin{lstlisting}[language=bash]
# Developer workflow:
$ git checkout -b feature/new_kernel_e
$ nano Python/kernels/kernel_e.py
$ ./tests/scripts/tests_start.sh --lint    # Fast
$ ./tests/scripts/tests_start.sh --execute # Real test
$ git add . && git commit -m "Add Kernel E"

# Pre-push: full audit
$ ./tests/scripts/tests_start.sh --force --all
\end{lstlisting}

Cache is not version-controlled (lives in \texttt{tests/.scope\_cache.json}, typically \texttt{.gitignored}). Each environment maintains its own cache.

\subsection{CI/CD Integration Points}

The system is designed for seamless CI/CD integration:

\begin{itemize}[noitemsep]
    \item \textbf{Exit codes:} 0 (success), 1 (failure) follow Unix conventions
    \item \textbf{JSON reports:} Structured output for metrics dashboards
    \item \textbf{Markdown reports:} Human-readable for review systems (GitHub Actions, GitLab CI)
    \item \textbf{Deterministic:} Fixed PRNG seeds ensure test reproducibility
\end{itemize}

Typical CI pipeline:

\begin{lstlisting}[language=bash]
# .github/workflows/test.yml
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.13'
      - run: pip install -r requirements.txt
      - run: ./Test/scripts/tests_start.sh --force --all
      - uses: actions/upload-artifact@v3
        with:
          name: test-reports
          path: Test/reports/
\end{lstlisting}

% ============================================================================
\section{Usage Patterns}
% ============================================================================

\subsection{Development Workflow}

Developers use different invocations depending on context:

\subsubsection{Local Development (Quick Feedback)}

\begin{lstlisting}[language=bash]
# Edit a file
$ nano Python/api/config.py

# Run only affected tests (incremental, ~2s)
$ ./tests/scripts/tests_start.sh --lint

# Fix issues, test again
$ ./tests/scripts/tests_start.sh

# Run full execution tests when ready
$ ./tests/scripts/tests_start.sh --execute
\end{lstlisting}

\subsubsection{Pre-Commit Hook}

\begin{lstlisting}[language=bash]
# Create .git/hooks/pre-commit
#!/bin/bash
set -e
./Test/scripts/tests_start.sh --lint
echo "✓ Pre-commit checks passed"
\end{lstlisting}

\subsubsection{Full Audit Before Release}

\begin{lstlisting}[language=bash]
# Reset cache and run everything
$ ./tests/scripts/tests_start.sh --force --all
$ echo $?  # Exit code 0 = safe to release
\end{lstlisting}

\subsubsection{Debugging Failed Tests}

\begin{lstlisting}[language=bash]
# Run just execution tests with verbose output
$ ./tests/scripts/tests_start.sh --execute -v

# View detailed report
$ cat tests/reports/code_structure_last.md

# Parse JSON for programmatic inspection
$ python -c "import json; \
    r = json.load(open('tests/results/code_structure_last.json')); \
    print(r['failed_checks'])"
\end{lstlisting}

\subsection{CI/CD Workflow}

Automated systems run in \textbf{full-audit mode} always:

\begin{lstlisting}[language=bash]
# CI job always: --force (full audit)
./tests/scripts/tests_start.sh --force --all

# Parse results for downstream actions
if [ $? -ne 0 ]; then
    echo "Tests failed!"
    exit 1
fi

# Post results to dashboard
curl -X POST https://dashboard.example.com/test-results \
    -H "Content-Type: application/json" \
    -d @tests/results/code_structure_last.json
\end{lstlisting}

\subsection{Troubleshooting Using Reports}

Practitioners use JSON and Markdown reports for debugging:

\begin{lstlisting}[language=bash]
# Find which test failed
grep -A 5 '"passed": false' Test/results/code_structure_last.json

# Short summary
head -20 Test/reports/code_lint_last.md

# All compliance failures
grep "FAIL" Test/reports/code_alignement_last.md
\end{lstlisting}

% ============================================================================
\section{Expected Outcomes}
% ============================================================================

\subsection{Quality Metrics}

The testing system maintains these minimum quality thresholds:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Metric} & \textbf{Target} & \textbf{Current Status} \\
\hline
Policy Compliance & 36/36 (100\%) & ✓ Passing \\
\hline
Linting (flake8) & 0 violations & ✓ Passing \\
\hline
Code Formatting (black) & 0 issues & ✓ Passing \\
\hline
Import Ordering (isort) & 100\% compliant & ✓ Passing \\
\hline
Type Checking (mypy) & \textit{Informational} & Recommended to fix \\
\hline
Unit Tests & 79/79 passing & ✓ Passing \\
\hline
JAX Determinism & Fixed PRNG seeds & ✓ Verified \\
\hline
64-bit Precision & All operations & ✓ Enabled \\
\hline
\end{tabular}
\end{center}

\subsection{Performance Expectations}

Expected test execution times on standard development machine (MacBook Pro M1):

\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Scenario} & \textbf{Linting} & \textbf{Compliance} & \textbf{Execution} \\
\hline
Full audit & 8.2s & 12.5s & 24.3s \\
Incremental (2 files) & 2.1s & 12.5s & 24.3s \\
Incremental (0 files) & 0.8s & 12.5s & 24.3s \\
\hline
\textbf{Cumulative Total} & 45.0s & 39.0s &  37.8s \\
\hline
\end{tabular}

\noindent\textsuperscript{Note:} Linting scales with number of changed files; compliance and execution are repository-wide (not file-specific).
\end{center}

\subsection{Success Criteria}

A successful test run satisfies all criteria:

\begin{enumerate}
    \item \textbf{Exit Code 0:} All phases completed without failure
    \item \textbf{All Policies Pass:} All 36 architecture policies verified
    \item \textbf{Linting Green:} No PEP 8, formatting, or import violations
    \item \textbf{All Tests Pass:} 79/79 unit tests execute successfully
    \item \textbf{Reports Generated:} Both JSON and Markdown reports created
    \item \textbf{Deterministic:} Same results for same code (reproducible)
\end{enumerate}

\subsection{Failure Analysis}

When tests fail, use this decision tree:

\begin{enumerate}
    \item \textbf{Linting Failed:}
    \begin{itemize}[noitemsep]
        \item \textbf{flake8 error:} Refactor code to remove violations
        \item \textbf{black/isort error:} Run auto-fix: \texttt{black Python/ \&\& isort Python/}
    \end{itemize}
    
    \item \textbf{Compliance Failed:}
    \begin{itemize}[noitemsep]
        \item Read policy number from report
        \item Consult \texttt{code\_audit\_policies.tex} for requirement
        \item Modify code/config to satisfy policy
    \end{itemize}
    
    \item \textbf{Execution Failed:}
    \begin{itemize}[noitemsep]
        \item Review pytest output for failing test class/method
        \item Check test expectations in \texttt{code\_structure.py}
        \item Verify JAX operations and numerical precision
        \item Run test locally with verbose output: \texttt{pytest -v -s}
    \end{itemize}
\end{enumerate}

\subsection{Continuous Improvement}

The system is designed for iterative refinement:

\begin{itemize}[noitemsep]
    \item \textbf{Add new policy:} Update \texttt{policy\_checks()} in \texttt{code\_alignement.py}
    \item \textbf{Add new test:} Create test class in \texttt{code\_structure.py}; use \texttt{scope\_discovery} for module enumeration
    \item \textbf{Add new linter:} Extend \texttt{code\_lint.py} with new function returning \texttt{LinterResult}
    \item \textbf{Optimize performance:} Parallelization hooks exist for future enhancement
\end{itemize}

% ============================================================================
\section{Logical Architecture \& Execution Flow}
% ============================================================================

\subsection{Two-Layer Validation Model}

The testing system implements a refined \textbf{2-layer validation architecture} with centralized orchestration:

\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Layer} & \textbf{Script} & \textbf{Purpose} & \textbf{Discovery} \\
\hline
1. Compliance & \texttt{code\_alignement.py} & Validate 36 policies & Auto-discover modules \\
\hline
2. Execution & \texttt{code\_structure.py} & Real tests with pytest+JAX & Auto-discover + parametrize \\
\hline
\end{tabular}
\end{center}

\noindent\textbf{Note:} The deprecated \texttt{tests\_coverage.py} was consolidated into \texttt{code\_structure.py} via the \texttt{TestIOModuleImportable} test class to avoid redundancy.

\subsection{Scope Auto-Discovery Pattern}

Central to the architecture is the \textbf{Dynamic Scope Discovery} system implemented in \texttt{scope\_discovery.py}. This module provides functions used by all test scripts:

\begin{lstlisting}[language=Python]
# Auto-discover all modules in Python/ directory
def discover_modules(root) -> List[str]:
    return ['api', 'core', 'kernels', 'io']

# Extract public API from module's __init__.py
def extract_public_api(module_name) -> Set[str]:
    return {'PredictorConfig', 'initialize_jax_prng', ...}

# Get all modules' public APIs
def discover_all_public_api() -> Dict[str, Set[str]]:
    return {'api': {...}, 'core': {...}, ...}
\end{lstlisting}

\subsubsection{Benefits of Auto-Discovery}

\begin{itemize}[noitemsep]
    \item \textbf{Zero Hardcoding:} No manual lists of modules or functions
    \item \textbf{Automatic Adaptation:} Detects new modules when added to \texttt{Python/}
    \item \textbf{Reduced Maintenance:} Scope changes propagate across all scripts automatically
    \item \textbf{Consistency:} All test phases use identical discovery logic
\end{itemize}

\subsection{Change Detection Mechanism}

\subsubsection{Motivation}

In production workflows, code changes are localized (typically 1-5 files per commit). Full test audits on every commit waste computational resources. The system includes intelligent \textbf{change tracking} to run only affected tests:

\begin{center}
\begin{tabular}{|l|r|r|}
\hline
\textbf{Scenario} & \textbf{Files} & \textbf{Time} \\
\hline
Incremental (2 files changed) & 2/28 & $\sim$3s \\
Incremental (no changes) & 0/28 & $\sim$1s \\
Full audit (\texttt{--force}) & 28/28 & $\sim$45s \\
\hline
\end{tabular}
\end{center}

\subsubsection{Implementation}

The cache file \texttt{Test/.scope\_cache.json} stores modification timestamps of all Python files:

\begin{lstlisting}[language=Python]
# From scope_discovery.py
def discover_changed_files(root=None, force_all=False) -> List[str]:
    # Load cache from last run
    previous = load_file_timestamps()
    
    # Get current state
    current = get_all_python_files()
    
    # Compare: include file if new or timestamp changed
    if force_all:
        changed = list(current.keys())
    else:
        changed = [f for f in current 
                  if f not in previous or current[f] != previous[f]]
    
    # Update cache for next run
    save_file_timestamps(current)
    return changed
\end{lstlisting}

\subsubsection{CLI Flags}

\begin{table}[h]
\centering
\begin{tabular}{|l|p{7cm}|l|}
\hline
\textbf{Flag} & \textbf{Behavior} & \textbf{Cache} \\
\hline
(default) & Process only modified files (code\_alignement) & Persistent \\
\texttt{--force-all} & Process all files, update cache & Updated with all files \\
\texttt{--no-cache} & Disable cache (same as --force-all) & Updated but not used \\
\texttt{--reset-cache} & Clear cache without running tests & Cleared \\
\hline
\end{tabular}
\caption{Cache control flags for code\_alignement.py and code\_structure.py}
\end{table}

\noindent\textbf{Note:} \texttt{code\_alignement.py} uses cache to skip unchanged files (10-50$\times$ speedup). \texttt{code\_structure.py} reports changed files but runs all tests due to pytest fixture dependencies.

\subsection{Pipeline Execution Flow}

The complete execution follows this sequence:

\begin{enumerate}
    \item \textbf{Environment Validation}
    \begin{itemize}[noitemsep]
        \item Verify \texttt{.venv/bin/python} exists
        \item Verify \texttt{Test/results/} and \texttt{Test/reports/} directories
        \item Initialize cache system from \texttt{Test/.scope\_cache.json}
    \end{itemize}
    
    \item \textbf{Stage 0: Dependency Version Check} (\texttt{dependency\_check.py})
    \begin{itemize}[noitemsep]
        \item Parse \texttt{requirements.txt} with PEP 508 environment marker support
        \item Detect current platform (\texttt{sys.platform}, \texttt{platform.machine})
        \item Compare installed versions against Golden Master specification
        \item Generate JSON and Markdown reports
        \item \textbf{If FAIL} $\rightarrow$ Stop pipeline immediately (environment mismatch)
    \end{itemize}
    
    \item \textbf{Stage 1: Code Linting} (\texttt{code\_lint.py})
    \begin{itemize}[noitemsep]
        \item Run flake8, black, isort, mypy on incremental (changed) files
        \item Generate JSON and Markdown reports
        \item \textbf{If FAIL} $\rightarrow$ Stop pipeline (fail-fast)
    \end{itemize}
    
    \item \textbf{Stage 2: Policy Compliance} (\texttt{code\_alignement.py})
    \begin{itemize}[noitemsep]
        \item Validate 38 CODE\_AUDIT\_POLICIES across entire repository
        \item Generate detailed compliance reports
        \item \textbf{If FAIL} $\rightarrow$ Stop pipeline (fail-fast)
    \end{itemize}
    
    \item \textbf{Stage 3: Execution Tests} (\texttt{code\_structure.py})
    \begin{itemize}[noitemsep]
        \item Run pytest suite with unit tests
        \item Real JAX execution with 64-bit precision
        \item Generate test results and coverage reports
    \end{itemize}
    
    \item \textbf{Summary}
    \begin{itemize}[noitemsep]
        \item Total/Passed/Failed counts per stage
        \item List latest artifacts in \texttt{Test/results/}
        \item Return aggregated exit code (0 = all passed, 1 = any failed)
    \end{itemize}
\end{enumerate}

\subsection{Report Generation}

Each test script produces two output formats for maximum compatibility:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Script} & \textbf{JSON Report} & \textbf{Markdown Report} \\
\hline
\texttt{code\_lint.py} & \texttt{code\_lint\_last.json} & \texttt{code\_lint\_last.md} \\
\texttt{code\_alignement.py} & \texttt{code\_alignement\_last.json} & \texttt{code\_alignement\_last.md} \\
\texttt{code\_structure.py} & \texttt{code\_structure\_last.json} & \texttt{code\_structure\_last.md} \\
\hline
\end{tabular}
\end{center}

\noindent All reports use \texttt{\_last} suffix (no timestamp) to enable scripted access in CI/CD pipelines. JSON format includes structured failure data for dashboard integration; Markdown format is human-readable for manual review.

\subsection{Architecture Evolution Note}

\subsubsection{Previous: 3-Layer Model}

The system previously used three validation stages:
\begin{enumerate}
    \item \texttt{code\_alignement.py} (policies)
    \item \texttt{tests\_coverage.py} (coverage)
    \item \texttt{code\_structure.py} (execution)
\end{enumerate}

\subsubsection{Identified Redundancy}

Since \texttt{code\_structure.py} auto-generates tests for all discovered functions via \texttt{TestIOModuleImportable}, a separate coverage stage was logically redundant: all public functions are already tested.

\subsubsection{Current: 2-Layer Model}

The coverage stage was consolidated into \texttt{code\_structure.py}, resulting in:
\begin{itemize}[noitemsep]
    \item Simpler pipeline (fewer stages)
    \item No redundant validation
    \item Faster overall test execution
\end{itemize}

% ============================================================================
\section{Core Components}
% ============================================================================

\subsection{Orchestrator: \texttt{Test/scripts/tests\_start.sh}}

\subsubsection{Purpose}

Central orchestration script that:
\begin{itemize}[noitemsep]
    \item Detects virtual environment and Python executable
    \item Parses CLI options (\texttt{--force-all} for full audit, \texttt{--lint}, \texttt{--execute})
    \item Manages test sequencing and fail-fast logic
    \item Generates unified summary reports
\end{itemize}

\subsubsection{Invocation}

\begin{lstlisting}[language=bash]
./tests/scripts/tests_start.sh [OPTION]

# Examples
./tests/scripts/tests_start.sh              # Default: all tests, changed files only
./tests/scripts/tests_start.sh --force      # Full audit: reset cache, all files
./tests/scripts/tests_start.sh --lint       # Only: code linting
./tests/scripts/tests_start.sh --compliance # Only: policy audit
./tests/scripts/tests_start.sh --execute    # Only: functional tests
./tests/scripts/tests_start.sh --reset-cache # Clean cache, exit
\end{lstlisting}

\subsubsection{Implementation Details}

\begin{itemize}[noitemsep]
    \item \textbf{Python Detection:} Prefers \texttt{.venv/bin/python}, falls back to system Python
    \item \textbf{Change Tracking:} Calls \texttt{reset\_cache()} if \texttt{--force} flag present
    \item \textbf{Fail-Fast:} Each test may pass extra arguments (e.g., \texttt{--force-all})
    \item \textbf{Artifact Management:} Creates \texttt{tests/results/} and \texttt{tests/reports/} directories
\end{itemize}

\subsubsection{Exit Codes}

\begin{center}
\begin{tabular}{|c|l|}
\hline
\textbf{Code} & \textbf{Meaning} \\
\hline
0 & All tests passed \\
1 & Test failure (first failure stops pipeline) \\
2 & Invalid CLI argument \\
127 & Python virtual environment not found \\
\hline
\end{tabular}
\end{center}

% ============================================================================

\subsection{Linting Phase: \texttt{code\_lint.py}}

\subsubsection{Purpose}

Python code quality validation using four industry-standard tools:

\begin{enumerate}
    \item \textbf{flake8:} PEP 8 style violations, complexity checks
    \item \textbf{black:} Code formatting audit (not auto-fix in CI)
    \item \textbf{isort:} Import statement ordering
    \item \textbf{mypy:} Static type annotation checking (informational)
\end{enumerate}

\subsubsection{Scope}

Scans all \texttt{*.py} files in \texttt{Python/} directory recursively, automatically discovering modules \texttt{api}, \texttt{core}, \texttt{kernels}, \texttt{io}.

\subsubsection{Configuration}

\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Tool} & \textbf{Configuration} \\
\hline
flake8 & \texttt{--max-line-length=120}, ignore E203/W503/E501 \\
black & \texttt{--line-length=120}, check mode \\
isort & \texttt{--profile=black}, PEP 8 compliant \\
mypy & \texttt{--ignore-missing-imports}, silent import mode \\
\hline
\end{tabular}
\end{center}

\subsubsection{Output}

Generates two reports in standardized formats:

\begin{itemize}[noitemsep]
    \item \textbf{JSON:} \texttt{tests/results/code\_lint\_last.json}
    \begin{itemize}[noitemsep]
        \item Timestamp, total checks, pass/fail counts
        \item Per-tool results with file counts and violation details
    \end{itemize}
    
    \item \textbf{Markdown:} \texttt{tests/reports/code\_lint\_last.md}
    \begin{itemize}[noitemsep]
        \item Human-readable table format
        \item Auto-fix recommendations for formatting issues
    \end{itemize}
\end{itemize}

\subsubsection{Exit Behavior}

\begin{itemize}[noitemsep]
    \item \textbf{Mandatory tools} (flake8, black, isort): Return 0 only if all pass
    \item \textbf{Informational} (mypy): Report findings but do not fail pipeline
    \item Fail-fast: Stop orchestrator if any mandatory tool fails
\end{itemize}

\subsubsection{Change Detection Integration}

By default, processes only files detected as modified since last cache. Use \texttt{--force-all} to scan all 28 files:

\begin{lstlisting}[language=python]
from tests.scripts.scope_discovery import discover_changed_files
changed = discover_changed_files(force_all=False)  # Incremental
all_files = discover_changed_files(force_all=True)  # Full audit
\end{lstlisting}

% ============================================================================

\subsection{Compliance Phase: \texttt{code\_alignement.py}}

\subsubsection{Purpose}

Repository-wide audit of 36 architecture policies derived from the specification corpus. Policies cover:

\begin{itemize}[noitemsep]
    \item Configuration sourcing and immutability (Policies 1--5)
    \item Numerical stability and precision (Policies 11--12)
    \item Kernel purity and JAX constraints (Policies 13, 36--38)
    \item I/O safety: snapshots, telemetry, credentials (Policies 18--21, 30--31)
    \item Advanced settings: entropy, Sinkhorn, DGM (Policies 23--26)
\end{itemize}

\subsubsection{Scope}

Repository-wide. Policies apply to:
\begin{itemize}[noitemsep]
    \item Configuration files (\filehref{config.toml})
    \item Python initialization files (\texttt{\_\_init\_\_.py})
    \item Core module implementations
    \item Documentation and specification corpus
\end{itemize}

\subsubsection{Verification Method}

Each policy is implemented as a closure returning \texttt{(bool, str)} tuple:

\begin{lstlisting}[language=python]
def policy_checks() -> List[Tuple[int, str, Callable[[], Tuple[bool, str]]]]:
    return [
        (1, "Configuration Sourcing", lambda: check_config_sourcing()),
        (2, "Configuration Immutability", lambda: check_immutability()),
        # ... 34 more policies
    ]
\end{lstlisting}

\subsubsection{Output}

\begin{itemize}[noitemsep]
    \item \textbf{JSON:} \texttt{Test/results/code\_alignement\_last.json}
    \begin{itemize}[noitemsep]
        \item Timestamp, policy count, pass/fail breakdown
        \item Per-policy result with details
        \item Cache status: incremental vs full scan
    \end{itemize}
    
    \item \textbf{Markdown:} \texttt{Test/reports/code\_alignement\_last.md}
    \begin{itemize}[noitemsep]
        \item Policy table with pass/fail status
        \item Detailed failure messages for debugging
        \item Files checked count (incremental mode)
    \end{itemize}
\end{itemize}

\subsubsection{Exit Behavior}

\begin{itemize}[noitemsep]
    \item Returns 0 only if all 36 policies pass
    \item Fails entire pipeline if any policy violated
    \item Non-fatal: Continues checking all policies (doesn't stop at first failure)
\end{itemize}

% ============================================================================

\subsection{Execution Phase: \texttt{code\_structure.py}}

\subsubsection{Purpose}

Functional unit testing via pytest framework. Executes 79 parametrized tests covering:

\begin{itemize}[noitemsep]
    \item \textbf{API Module (26 tests):} Config, PRNG, types, validation, schemas, state buffer, warmup
    \item \textbf{Core Module (7 tests):} Orchestrator, fusion, meta-optimizer
    \item \textbf{Kernels Module (18 tests):} Each kernel (A/B/C/D), base functions
    \item \textbf{I/O Module (7 tests):} Module importability checks
    \item \textbf{Integration (10 tests):} Warmup sequences, profile, retry logic
\end{itemize}

\subsubsection{Framework}

\begin{itemize}[noitemsep]
    \item \textbf{Framework:} pytest 7.3.0 with hypothesis (property-based testing)
    \item \textbf{JAX Integration:} Real JAX operations with 64-bit precision enabled
    \item \textbf{Determinism:} All tests use fixed PRNG seeds for reproducibility
    \item \textbf{Isolation:} Each test receives fresh config and random key
\end{itemize}

\subsubsection{Test Organization}

Tests are grouped into classes for organization:

\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Test Class} & \textbf{Count} \\
\hline
TestBasicSetup & 2 \\
TestAPIConfig & 4 \\
TestAPIPRNG & 6 \\
TestAPITypes & 4 \\
TestAPIValidation & 12 \\
TestAPISchemas & 2 \\
TestAPIStateBuffer & 5 \\
TestCoreOrchestrator & 6 \\
TestCoreFusion & 1 \\
TestCoreMetaOptimizer & 1 \\
TestKernelsBase & 4 \\
TestKernelA & 4 \\
TestKernelB & 3 \\
TestKernelC & 2 \\
TestKernelD & 3 \\
TestAPIWarmup & 7 \\
TestIOModuleImportable & 7 \\
\hline
\textbf{Total} & \textbf{79} \\
\hline
\end{tabular}
\end{center}

\subsubsection{Output}

\begin{itemize}[noitemsep]
    \item \textbf{Console:} pytest-formatted output with pass/fail summary and cache status
    \item \textbf{JSON:} \texttt{Test/results/code\_structure\_last.json} (structured results)
    \item \textbf{Markdown:} \texttt{Test/reports/code\_structure\_last.md} (detailed summary with coverage)
\end{itemize}

\subsubsection{Exit Behavior}

\begin{itemize}[noitemsep]
    \item Returns 0 only if all 79 tests pass
    \item Returns 1 if any test fails (fail-fast not applied; all tests run)
    \item Captures full traceback for failed assertions
\end{itemize}

% ============================================================================

\subsection{Discovery Module: \texttt{scope\_discovery.py}}

\subsubsection{Purpose}

Provides dynamic module discovery and change detection without hardcoding paths or module lists. Enables:

\begin{enumerate}
    \item \textbf{Auto-Discovery:} Enumerate modules from \texttt{Python/} directory structure
    \item \textbf{Change Tracking:} Detect modified files since last test run
    \item \textbf{Incremental Scanning:} Process only changed files for speed
    \item \textbf{Public API Extraction:} Parse module \texttt{\_\_init\_\_.py} for public symbols
\end{enumerate}

\subsubsection{Core Functions}

\begin{center}
\begin{tabular}{|p{4cm}|p{6cm}|}
\hline
\textbf{Function} & \textbf{Purpose} \\
\hline
\texttt{discover\_modules()} & List all modules in \texttt{Python/}: ['api', 'core', 'kernels', 'io'] \\
\hline
\texttt{discover\_module\_files()} & List .py files in a specific module \\
\hline
\texttt{extract\_public\_api()} & Parse module's \texttt{\_\_all\_\_} or extract public symbols \\
\hline
\texttt{discover\_all\_public\_api()} & Return dict of all modules' public APIs \\
\hline
\texttt{get\_all\_python\_files()} & Get all .py files with modification timestamps \\
\hline
\texttt{discover\_changed\_files(force\_all)} & Return files modified since last cache, or all if \texttt{force\_all=True} \\
\hline
\texttt{discover\_module\_files\_changed()} & Discover changed files in specific module \\
\hline
\texttt{load\_file\_timestamps()} & Load cached file timestamps from \texttt{.scope\_cache.json} \\
\hline
\texttt{save\_file\_timestamps()} & Update cache with current file state \\
\hline
\texttt{reset\_cache()} & Delete cache (forces full scan on next run) \\
\hline
\texttt{get\_cache\_info()} & Return cache statistics \\
\hline
\end{tabular}
\end{center}

\subsubsection{Change Detection Algorithm}

Algorithm for detecting file changes:

\begin{enumerate}
    \item \textbf{Get Current State:} Scan all .py files, store path + modification timestamp
    \item \textbf{Load Previous State:} Read timestamps from \texttt{tests/.scope\_cache.json}
    \item \textbf{Compare:} For each file:
    \begin{itemize}[noitemsep]
        \item If not in previous cache: File is NEW → Include
        \item If mtime differs: File is MODIFIED → Include  
        \item If mtime unchanged: File not changed → Omit
    \end{itemize}
    \item \textbf{Save New State:} Update cache for next run
    \item \textbf{Return:} List of changed files
\end{enumerate}

\subsubsection{Cache Format}

Cache file: \texttt{Test/.scope\_cache.json}

\begin{lstlisting}
{
  "Python/api/__init__.py": 1708456789.123,
  "Python/api/config.py": 1708456795.456,
  "Python/api/prng.py": 1708456801.789,
  "Python/core/orchestrator.py": 1708456812.345,
  ...
}
\end{lstlisting}

\begin{tabular}{|l|l|}
\hline
\textbf{Key} & Relative path from project root (28 Python files total) \\
\textbf{Value} & Unix timestamp (float with microsecond precision from st\_mtime) \\
\hline
\end{tabular}

\noindent\textbf{Coverage:} Cache automatically ignored by \texttt{*.json} rule in .gitignore. Detection threshold: 0.001 seconds (prevents false negatives).

\subsubsection{Performance Characteristics}

\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Scenario} & \textbf{Files} & \textbf{Time} & \textbf{Cache?} \\
\hline
Incremental (2 files changed) & 2/28 & $\approx$3s & Yes \\
No changes since last run & 0/28 & $\approx$1s & Yes \\
Full audit (\texttt{--force}) & 28/28 & $\approx$45s & Reset \\
\hline
\end{tabular}
\end{center}

% ============================================================================
\section{Change Detection System}
% ============================================================================

\subsection{Motivation}

During development, most changes are localized to 1-3 files. Running all tests on all files is inefficient:

\begin{itemize}[noitemsep]
    \item Full test suite: $\approx$45s (lint + compliance + execution)
    \item Typical workflow: 3s with incremental detection
    \item \textbf{Speedup:} 15$\times$ for common case
\end{itemize}

\subsection{User Interface}

\subsubsection{Incremental (Default)}

\begin{lstlisting}[language=bash]
$ ./tests/scripts/tests_start.sh
Mode: Incremental (changed files only)
Changed files: 2 (api/config.py, core/fusion.py)
RUNNING: Code Linting Checks
✓ PASSED

# Next run (no changes)
$ ./tests/scripts/tests_start.sh
Changed files: 0
RUNNING: Code Linting Checks
✓ PASSED (trivial, no files to check)
\end{lstlisting}

\subsubsection{Full Audit}

\begin{lstlisting}[language=bash]
$ ./tests/scripts/tests_start.sh --force
Mode: FULL AUDIT (all files, cache reset)
Changed files: 28 (all)
RUNNING: Code Linting Checks
...
RUNNING: Policy Compliance Check
...
RUNNING: Code Structure Execution Tests
✓ ALL TESTS PASSED
\end{lstlisting}

\subsubsection{Cache Management}

\begin{lstlisting}[language=bash]
# View cache status
python -c "from tests.scripts.scope_discovery import get_cache_info; \
          import json; print(json.dumps(get_cache_info(), indent=2))"
# Output: {"cached_files": 28, "cache_size_bytes": 1369, "exists": true}

# Reset cache
./tests/scripts/tests_start.sh --reset-cache
# Output: ✓ Cache reset

# Or programmatically
python -c "from tests.scripts.scope_discovery import reset_cache; reset_cache()"
\end{lstlisting}

\subsection{Integration with Scripts}

Each test script can optionally receive \texttt{--force-all} to bypass change detection:

\begin{lstlisting}[language=python]
# code_lint.py
if __name__ == "__main__":
    force_all = "--force-all" in sys.argv
    changed = discover_changed_files(force_all=force_all)
    # Process changed files...
\end{lstlisting}

% ============================================================================
\section{Execution Flow}
% ============================================================================

\subsection{Complete Pipeline Diagram}

The following describes the complete flow from invocation to results:

\begin{enumerate}
    \item \textbf{User invokes:} \texttt{./tests/scripts/tests\_start.sh [OPTIONS]}
    
    \item \textbf{Orchestrator prepares:}
    \begin{itemize}[noitemsep]
        \item Verify .venv exists
        \item Parse CLI options
        \item Reset cache if \texttt{--force}
        \item Create \texttt{tests/results/} and \texttt{tests/reports/} directories
    \end{itemize}
    
    \item \textbf{Test Phase 1 - Linting:}
    \begin{itemize}[noitemsep]
        \item \texttt{code\_lint.py} discovers changed files (or all if \texttt{--force-all})
        \item Runs: flake8, black, isort, mypy
        \item Generates: JSON result, Markdown report, console output
        \item Fails if mandatory tool fails
    \end{itemize}
    
    \item \textbf{If Phase 1 passes, Test Phase 2 - Compliance:}
    \begin{itemize}[noitemsep]
        \item \texttt{code\_alignement.py} executes 36 policy checks
        \item Policies are repository-wide (not file-specific)
        \item Generates: JSON result, Markdown report, console output
        \item Fails if any policy violated
    \end{itemize}
    
    \item \textbf{If Phase 2 passes, Test Phase 3 - Execution:}
    \begin{itemize}[noitemsep]
        \item \texttt{code\_structure.py} runs 79 unit tests via pytest
        \item Uses real JAX with 64-bit precision
        \item Generates: JSON result, Markdown report, pytest output
        \item Fails if any test assertion fails
    \end{itemize}
    
    \item \textbf{After all phases:}
    \begin{itemize}[noitemsep]
        \item Print unified summary
        \item List generated artifacts
        \item Return exit code (0 = success, 1 = failure)
    \end{itemize}
\end{enumerate}

\subsection{Fail-Fast Semantics}

Pipeline stops on first failure:

\begin{lstlisting}[language=bash]
$ ./tests/scripts/tests_start.sh
RUNNING: Code Linting Checks
✗ FAILED (exit code: 1)
⚠️ Stopping execution: linting checks failed
EXIT CODE: 1

# Phases 2 and 3 do NOT run
\end{lstlisting}

% ============================================================================
\section{Reporting and Artifacts}
% ============================================================================

\subsection{Directory Structure}

\begin{lstlisting}
tests/
├── scripts/
│   ├── tests_start.sh           # Orchestrator
│   ├── dependency_check.py      # Dependency validation (Stage 0)
│   ├── code_lint.py             # Linting phase (Stage 1)
│   ├── code_alignement.py       # Compliance phase (Stage 2)
│   ├── code_structure.py        # Execution phase (Stage 3)
│   ├── scope_discovery.py       # Discovery + change tracking
│   └── __init__.py
├── results/
│   ├── dependency_check_last.json # Latest dependency check results
│   ├── code_lint_last.json      # Latest linting results
│   ├── code_alignement_last.json # Latest compliance results
│   ├── code_structure_last.json  # Latest execution results
│   └── .scope_cache.json         # Change detection cache
└── reports/
    ├── dependency_check_last.md  # Human-readable dependency report
    ├── code_lint_last.md         # Human-readable linting report
    ├── code_alignement_last.md   # Human-readable compliance report
    └── code_structure_last.md    # Human-readable execution report
\end{lstlisting}

\subsection{Report Formats}

\subsubsection{JSON Format}

All JSON reports follow a consistent structure:

\begin{lstlisting}
{
  "timestamp": "2026-02-21 14:32:15 UTC",
  "total_checks": 36,
  "passed_checks": 36,
  "failed_checks": 0,
  "results": [
    {
      "tool": "flake8",
      "passed": true,
      "details": "Checked 28 files"
    },
    ...
  ]
}
\end{lstlisting}

Enables programmatic CI/CD integration: parse JSON to extract pass/fail status.

\subsubsection{Markdown Format}

Human-readable format with:
\begin{itemize}[noitemsep]
    \item Timestamp and scope information
    \item Summary statistics
    \item Detailed results table
    \item Recommendations for failures
\end{itemize}

Example excerpt:

\begin{lstlisting}
# Code Quality Linting Report
**Generated:** 2026-02-21 14:32:15 UTC
**Result:** 4/4 checks passed

## Linting Results
| Tool | Status | Details | Files |
|------|--------|---------|-------|
| flake8 | ✓ PASS | Code style OK | 28 |
| black | ✓ PASS | Formatting OK | 28 |
| isort | ✓ PASS | Import ordering OK | 28 |
| mypy | ✓ PASS | Type checking OK | 28 |
\end{lstlisting}

\subsection{Log Retrieval}

\subsubsection{Most Recent Results}

\begin{lstlisting}[language=bash]
# View latest linting results (pretty-printed)
python -m json.tool tests/results/code_lint_last.json

# View latest compliance results
python -m json.tool tests/results/code_alignement_last.json

# View latest execution results
python -m json.tool tests/results/code_structure_last.json
\end{lstlisting}

\subsubsection{Markdown Reports}

\begin{lstlisting}[language=bash]
# Display in terminal
cat tests/reports/code_lint_last.md

# Open in your editor
code tests/reports/code_lint_last.md
\end{lstlisting}

% ============================================================================
\section{Command Reference}
% ============================================================================

\subsection{Complete CLI Options}

\begin{center}
\begin{tabular}{|p{3cm}|p{7cm}|}
\hline
\textbf{Option} & \textbf{Behavior} \\
\hline
(no option) & Run all phases: lint → compliance → execute (incremental) \\
\hline
\texttt{--all} & Explicit form of default behavior \\
\hline
\texttt{--force} & Reset cache, full audit (all files), all phases \\
\hline
\texttt{--lint} & Linting phase only (incremental) \\
\hline
\texttt{--compliance} & Compliance phase only \\
\hline
\texttt{--execute} & Execution phase only \\
\hline
\texttt{--reset-cache} & Clear change detection cache and exit (no tests run) \\
\hline
\texttt{--help} & Display usage information \\
\hline
\end{tabular}
\end{center}

\subsection{Practical Workflows}

\subsubsection{Developer Workflow (Iterative)}

\begin{lstlisting}[language=bash]
# Modify a few files
nano Python/api/config.py
nano Python/core/orchestrator.py

# Quick test (only changed files)
./tests/scripts/tests_start.sh --lint

# If linting passes, check compliance
./tests/scripts/tests_start.sh --compliance

# If compliance passes, run full test
./tests/scripts/tests_start.sh --execute
\end{lstlisting}

\subsubsection{Pre-Commit Workflow}

\begin{lstlisting}[language=bash]
# Before committing, run full audit
./tests/scripts/tests_start.sh --force

# If all pass, safe to commit
git add .
git commit -m "Feature X implementation"
\end{lstlisting}

\subsubsection{CI/CD Integration}

\begin{lstlisting}[language=bash]
#!/bin/bash
set -e  # Exit on error

# Always run full audit in CI
./tests/scripts/tests_start.sh --force --all

# Parse JSON results for metrics
python scripts/parse_test_results.py tests/results/code_lint_last.json
python scripts/parse_test_results.py tests/results/code_alignement_last.json
python scripts/parse_test_results.py tests/results/code_structure_last.json

echo "All tests passed! Safe to merge."
\end{lstlisting}

% ============================================================================
\section{Troubleshooting}
% ============================================================================

\subsection{Common Issues}

\subsubsection{Issue: Python virtual environment not found}

\textbf{Error Message:}
\begin{lstlisting}
ERROR: Python virtual environment not found at .venv/bin/python
\end{lstlisting}

\textbf{Solution:}
\begin{lstlisting}[language=bash]
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
./tests/scripts/tests_start.sh --reset-cache
\end{lstlisting}

\subsubsection{Issue: Cache inconsistent or corrupted}

\textbf{Symptom:} Tests report 0 files changed but you modified files.

\textbf{Solution:}
\begin{lstlisting}[language=bash]
./tests/scripts/tests_start.sh --reset-cache
./tests/scripts/tests_start.sh
\end{lstlisting}

\subsubsection{Issue: Linting reports false positives}

\textbf{Symptom:} \texttt{flake8} complaints that seem unrelated.

\textbf{Solution:}
\begin{lstlisting}[language=bash]
# Auto-fix formatting issues
python -m black Python/ --line-length=120
python -m isort Python/ --profile=black

# Re-run linting
./tests/scripts/tests_start.sh --lint
\end{lstlisting}

\subsubsection{Issue: Tests timeout}

\textbf{Symptom:} \texttt{pytest} or \texttt{mypy} hangs indefinitely.

\textbf{Solution:}
\begin{lstlisting}[language=bash]
# For pytest (default timeout: 60s per test)
./tests/scripts/tests_start.sh --execute

# For mypy (default timeout: 180s)
# These are longer on first run due to cache building
python -m mypy Python/ --ignore-missing-imports
\end{lstlisting}

\subsubsection{Issue: Memory errors during execution tests}

\textbf{Symptom:} JAX kernel tests crash with OOM (Out of Memory).

\textbf{Solution:}
\begin{lstlisting}[language=bash]
# Limit JAX memory allocation
export JAX_PLATFORM_NAME=cpu
export JAX_CPU_FALLBACK=true
./tests/scripts/tests_start.sh --execute
\end{lstlisting}

% ============================================================================
\section{Implementation Notes}
% ============================================================================

\subsection{Design Decisions}

\subsubsection{Why Auto-Discovery?}

Hardcoding module lists creates maintenance burden:
\begin{itemize}[noitemsep]
    \item Adding new module requires updating multiple test scripts
    \item Risk of stale lists causing missed tests
    \item Violates DRY principle
\end{itemize}

\texttt{scope\_discovery.py} solves by:
\begin{itemize}[noitemsep]
    \item Scanning \texttt{Python/} for \texttt{\_\_init\_\_.py} files
    \item Dynamically building module list at runtime
    \item Works immediately for new modules (zero config)
\end{itemize}

\subsubsection{Why Change Detection?}

During development, running all 79 tests on every keystroke is excessive:
\begin{itemize}[noitemsep]
    \item Most changes are localized (1-3 files)
    \item Full test suite: 45s
    \item Typical workflow: 3s incremental
\end{itemize}

Implementation trade-offs:
\begin{itemize}[noitemsep]
    \item \textbf{Pros:} 15$\times$ speedup, instant feedback
    \item \textbf{Cons:} Cache can desync if external tools modify files
    \item \textbf{Mitigation:} \texttt{--force} and \texttt{--reset-cache} always available
\end{itemize}

\subsubsection{Why Separate Scripts?}

Three distinct phases (lint, compliance, execution) instead of monolithic script:
\begin{itemize}[noitemsep]
    \item \textbf{Modularity:} Each script has single responsibility
    \item \textbf{Reusability:} Can invoke scripts independently
    \item \textbf{Parallelization:} Future: run phases in parallel (with proper ordering)
    \item \textbf{Debugging:} Easier to isolate failures
\end{itemize}

\subsection{Extension Points}

\subsubsection{Adding New Linting Tool}

\begin{lstlisting}[language=python]
# In code_lint.py, add new function
def run_custom_linter() -> LinterResult:
    try:
        result = subprocess.run(
            ["custom-linter", PYTHON_DIR, ...],
            capture_output=True, timeout=60
        )
        return LinterResult(
            linter="custom",
            passed=(result.returncode == 0),
            violations=...,
            details=...
        )
    except Exception as e:
        return LinterResult(linter="custom", passed=False, ...)

# In main(), add to results list
results.append(run_custom_linter())
\end{lstlisting}

\subsubsection{Adding New Policy}

\begin{lstlisting}[language=python]
# In code_alignement.py, add to policy_checks()
(39, "My New Policy", lambda: check_my_new_policy()),

# Implement check function
def check_my_new_policy() -> Tuple[bool, str]:
    if condition_met():
        return True, "Policy satisfied"
    else:
        return False, "Policy violation: ..."
\end{lstlisting}

\subsubsection{Adding New Test Module}

New modules in \texttt{Python/} are automatically discovered:

\begin{lstlisting}
mkdir -p Python/mymodule
touch Python/mymodule/__init__.py
# Add tests to code_structure.py (search for "TestMyModule")

./Test/scripts/tests_start.sh --execute
# mymodule tests now included automatically!
\end{lstlisting}

% ============================================================================
\section{Performance Characteristics}
% ============================================================================

\subsection{Benchmarks}

Measurements on a typical development machine (MacBook Pro M1, 16GB RAM):

\begin{center}
\begin{tabular}{|l|r|r|r|r|}
\hline
\textbf{Phase} & \textbf{Full (--force-all)} & \textbf{Incr (2 files)} & \textbf{Incr (0 files)} & \textbf{Unit} \\
\hline
Linting & 8.2s & 2.1s & 0.8s & sec \\
Compliance & 12.5s & \$<\$1s\textsuperscript{*} & \$<\$0.5s\textsuperscript{*} & sec \\
Execution & 24.3s & 24.3s\textsuperscript{†} & 24.3s\textsuperscript{†} & sec \\
\hline
\textbf{Total} & 45.0s & 26.4s & 25.6s & sec \\
\hline
\end{tabular}
\end{center}

\noindent\textsuperscript{*} = \texttt{code\_alignement.py} now uses cache effectively: only checks changed files (13-50$\times$ speedup).

\noindent\textsuperscript{†} = \texttt{code\_structure.py} runs all 79 tests regardless (pytest fixture dependencies prevent selective execution).

\subsection{Optimization Opportunities}

\begin{enumerate}
    \item \textbf{Parallel Phases (Future):} Run lint + compliance in parallel (if dependencies allow)
    \item \textbf{Test Sharding:} Distribute 79 tests across multiple processes
    \item \textbf{Cached JAX Models:} Pre-compile JAX kernels (requires XLA cache management)
    \item \textbf{Type Cache:} mypy maintains cache, but rebuild on major changes
\end{enumerate}

% ============================================================================
\section{Future Enhancements}
% ============================================================================

\subsection{Planned Features}

\begin{enumerate}
    \item \textbf{Parallel Execution:} Run phases concurrently with proper dependency management
    \item \textbf{Coverage Reporting:} pytest-cov integration for code coverage metrics
    \item \textbf{Performance Tracking:} Historical benchmark comparisons
    \item \textbf{Pre-Commit Hooks:} Git integration to auto-run tests before commits
    \item \textbf{Web Dashboard:} Real-time test results with history
\end{enumerate}

\subsubsection{Known Limitations}

\begin{enumerate}
    \item \textbf{Change Detection:} Cannot detect changes made by external tools without filesystem mtime update (workaround: \texttt{--force-all})
    \item \textbf{code\_structure.py Cache:} Reports changed files but executes all tests (pytest fixture dependencies prevent selective execution)
    \item \textbf{Parallel Execution:} Not yet supported (would require refactoring orchestrator)
    \item \textbf{Remote Testing:} All tests assume local environment (no SSH/network testing)
    \item \textbf{Windows Support:} Shell scripts bash-specific (would need PowerShell port)
\end{enumerate}

% ============================================================================
\section{Conclusion}
% ============================================================================

The Universal Stochastic Predictor's testing infrastructure provides:

\begin{itemize}[noitemsep]
    \item \textbf{Three-phase validation:} Code quality → Policy compliance → Functional correctness
    \item \textbf{Zero-hardcoding:} Dynamic module discovery adapts to new modules automatically
    \item \textbf{Intelligent caching:} 15$\times$ faster typical workflow vs. full audit
    \item \textbf{Comprehensive reporting:} Machine-readable JSON + human-readable Markdown
    \item \textbf{Extensible design:} Easy to add tools, policies, and tests
\end{itemize}

This architecture supports both development-time rapid iteration and pre-release full audits, balancing productivity with rigor.

\vspace{2cm}

\begin{center}
Last Updated: February 21, 2026 \\
Test Framework Version: 2.1.0 \\
Specification Version: v2.1.0-Diamond-Spec
\end{center}

\end{document}
