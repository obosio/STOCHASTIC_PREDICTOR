\documentclass[11pt, a4paper]{report}

% --- PREÁMBULO ---
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{fontspec}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{amsthm}

\usepackage[spanish, provide=*]{babel}
\babelprovide[import, onchar=ids fonts]{spanish}

% Definición de fuente principal
% \babelfont{rm}{Noto Sans}

% Entornos
\newtheorem{definition}{Definición}[chapter]
\newtheorem{testcase}{Caso de Prueba}[chapter]
\newtheorem{criterion}{Criterio}[chapter]
\newtheorem{remark}{Nota}[chapter]

% --- HYPERREF (Debe ser el último paquete) ---
\usepackage[hidelinks]{hyperref}

\title{\textbf{Protocolo de Validación y Pruebas \\ del Predictor Estocástico Universal}}
\author{Consorcio de Desarrollo de Meta-Predicción Adaptativa}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\chapter{Pruebas de Unidad: Núcleos y Algoritmos Fundamentales}

Estas pruebas verifican la implementación aislada de los algoritmos críticos sin depender del estado global del sistema.

\section{Generación de Entropía y Variables Aleatorias}

\subsection{Algoritmo CMS}

\begin{testcase}[Validación de Distribuciones $\alpha$-Estables]
Validar que la generación de variables $\alpha$-estables mediante el método de Chambers-Mallows-Stuck produzca distribuciones con los parámetros $(\alpha, \beta, \gamma, \delta)$ deseados.
\end{testcase}

\begin{criterion}
Para una muestra de tamaño $N \geq 10^4$, los momentos empíricos deben coincidir con las propiedades teóricas de la distribución estable dentro de un intervalo de confianza del $95\%$.
\end{criterion}

\subsection{Integridad de Generadores Pseudo-Aleatorios}

\begin{testcase}[Mersenne Twister/PCG64]
Verificar la ausencia de correlaciones seriales y el cumplimiento de periodos largos en el generador de números pseudo-aleatorios.
\end{testcase}

\begin{criterion}
Aplicar las baterías de pruebas estadísticas estándar (TestU01, Diehard) y verificar que no se detecten fallos en los tests de aleatoriedad. El periodo del generador debe ser $\geq 2^{127}$ para Mersenne Twister y $\geq 2^{128}$ para PCG64.
\end{criterion}

\section{Análisis de Singularidad (SIA)}

\subsection{Detección del Exponente de Hölder}

\begin{testcase}[Validación de WTMM]
Utilizar señales sintéticas con exponente de Hölder conocido ($H$ determinado) para validar que el algoritmo WTMM (Wavelet Transform Modulus Maxima) recupere el espectro de singularidades $D(h)$ con un error $< 5\%$.
\end{testcase}

\begin{criterion}
Sea $f(t)$ una señal sintética con $H = H_0$ conocido. El exponente de Hölder estimado mediante WTMM debe satisfacer:
\[
|\hat{H} - H_0| < 0.05 \cdot H_0
\]
donde $\hat{H}$ es el valor estimado a través del análisis multiescala.
\end{criterion}

\subsection{Cono de Influencia}

\begin{testcase}[Radio de Influencia Besov]
Verificar que el enlace de máximos locales en el espacio de escalas respete el radio de influencia definido por $C_{besov}$.
\end{testcase}

\begin{criterion}
Para dos máximos consecutivos en escalas $s_1 < s_2$, sus posiciones temporales $(t_1, t_2)$ deben satisfacer:
\[
|t_2 - t_1| \leq C_{besov} \cdot (s_2 - s_1)
\]
donde $C_{besov}$ es la constante del espacio de Besov asociado a la wavelet utilizada.
\end{criterion}

\subsection{Validación del Límite de Nyquist Suave}

El documento de especificación de I/O establece una frecuencia mínima de inyección de datos para mantener la integridad del análisis multifractal. Este test valida explícitamente ese límite operacional.

\begin{testcase}[Aliasing Multifractal]
Reducir gradualmente la frecuencia de los datos de entrada hasta que el espectro $D(h)$ colapse, validando que el sistema detecte la condición de sub-muestreo antes de que se produzca degradación irreversible de la estimación.
\end{testcase}

\begin{criterion}
Considerar una señal multifractal de referencia con espectro de singularidades conocido $D_0(h)$ y frecuencia de muestreo nominal $f_0$. Realizar el siguiente protocolo:
\begin{enumerate}
    \item Reducir la frecuencia de muestreo mediante diezmado: $f_k = f_0 / 2^k$ con $k = 0, 1, 2, \ldots$
    \item Para cada $f_k$, calcular el espectro estimado $\hat{D}_k(h)$ mediante WTMM
    \item Calcular el error relativo de forma:
    \[
    \varepsilon_k = \frac{\|D_0(h) - \hat{D}_k(h)\|_{L^2}}{\|D_0(h)\|_{L^2}}
    \]
    \item Monitorear el exponente de Hölder dominante: $\hat{H}_k = \arg\max_h D_k(h)$
\end{enumerate}
El sistema debe cumplir:
\begin{itemize}
    \item \textbf{Detección preventiva:} Emitir señal \texttt{FreezingTopologicalBranchEvent} cuando $\varepsilon_k > 0.05$ (error $> 5\%$)
    \item \textbf{Criterio de aceptación:} La señal debe activarse \textit{antes} de que el error en la estimación de Hölder supere el $10\%$:
    \[
    \frac{|\hat{H}_k - H_0|}{H_0} < 0.10 \quad \text{en el momento de la congelación}
    \]
    \item \textbf{Acción correctiva:} Tras la señal, el peso de la Rama Topológica debe fijarse en su último valor confiable: $w_C \to w_C^{frozen}$ sin actualizaciones dinámicas
\end{itemize}
\end{criterion}

\begin{remark}
El límite de Nyquist en análisis multifractal no es una frontera rígida como en el muestreo de señales periódicas. La naturaleza autosimilar de los procesos multifractales permite cierta tolerancia al submuestreo, pero existe un \textit{soft-limit} por debajo del cual las estructuras de escala fina colapsan y el espectro $D(h)$ se degrada. Este test garantiza que el sistema opere siempre en el régimen de resolución adecuada.
\end{remark}

\begin{remark}
La frecuencia crítica $f_{critical}$ depende de:
\begin{enumerate}
    \item El rango de escalas $[s_{min}, s_{max}]$ de la transformada wavelet
    \item El soporte temporal de la wavelet madre $\psi(t)$
    \item La regularidad de Hölder mínima esperada $H_{min}$
\end{enumerate}
Como regla heurística, se recomienda:
\[
f_{min} \geq \frac{10}{s_{min}} \cdot (1 + H_{min}^{-1})
\]
Este criterio asegura al menos 10 puntos por escala mínima, ajustados por la rugosidad del proceso.
\end{remark}

\section{Estructuras Algebraicas (Rama D)}

\subsection{Identidad de Chen}

\begin{testcase}[Concatenación de Signaturas]
Validar que la concatenación de signaturas de dos segmentos de camino mediante el producto tensorial ($\otimes$) sea igual a la signatura del camino completo.
\end{testcase}

\begin{criterion}
Sean $\gamma_1: [0, T_1] \to \mathbb{R}^d$ y $\gamma_2: [0, T_2] \to \mathbb{R}^d$ dos caminos. La identidad de Chen establece:
\[
\text{Sig}(\gamma_1 \star \gamma_2) = \text{Sig}(\gamma_1) \otimes \text{Sig}(\gamma_2)
\]
donde $\gamma_1 \star \gamma_2$ denota la concatenación de caminos. El error numérico debe ser $< 10^{-6}$ en norma euclidiana.
\end{criterion}

\subsection{Invariancia de Escala}

\begin{testcase}[Reparametrización Temporal]
Comprobar que la signatura truncada al nivel $M$ sea invariante ante reparametrizaciones temporales del camino de entrada.
\end{testcase}

\begin{criterion}
Para una reparametrización estrictamente creciente $\phi: [0,1] \to [0,1]$ con $\phi(0)=0$ y $\phi(1)=1$, se debe cumplir:
\[
\text{Sig}^{(M)}(\gamma) = \text{Sig}^{(M)}(\gamma \circ \phi)
\]
donde $\text{Sig}^{(M)}$ denota la signatura truncada al nivel $M$.
\end{criterion}

\chapter{Pruebas de Integración y Convergencia Estocástica}

Validan que la interacción entre componentes respete las leyes de la probabilidad continua y las condiciones de estabilidad numérica.

\section{Solvers de Ecuaciones Diferenciales Estocásticas (SDE)}

\subsection{Convergencia de Esquemas Numéricos}

\begin{testcase}[Euler-Maruyama vs. Milstein]
Para una difusión con volatilidad no constante, verificar que el esquema de Milstein logre una convergencia fuerte de orden $1.0$ frente al orden $0.5$ de Euler-Maruyama.
\end{testcase}

\begin{criterion}
Considerar la EDE:
\[
dX_t = \mu(X_t, t)\, dt + \sigma(X_t, t)\, dW_t
\]
con $\sigma(x,t)$ no constante. Para una secuencia de pasos temporales $\Delta t_n = 2^{-n} \Delta t_0$, el error fuerte debe satisfacer:
\begin{align*}
E[|X_T - X_T^{EM}|^2] &= O(\Delta t^{0.5}) \quad \text{(Euler-Maruyama)} \\
E[|X_T - X_T^{M}|^2] &= O(\Delta t^{1.0}) \quad \text{(Milstein)}
\end{align*}
donde $X_T^{EM}$ y $X_T^{M}$ son las aproximaciones numéricas.
\end{criterion}

\subsection{Condición CFL Mixta}

\begin{testcase}[Violación de Estabilidad]
Forzar un paso de tiempo $\Delta t$ que viole la restricción de Courant-Friedrichs-Lewy y confirmar la aparición de inestabilidad numérica o valores no numéricos (NaN) como comportamiento esperado para calibrar el monitor de seguridad.
\end{testcase}

\begin{criterion}
Para un esquema explícito, la condición CFL requiere:
\[
\Delta t \leq \frac{C}{\sup_x |\mu'(x)| + \sigma^2(x)/2}
\]
Forzar $\Delta t > 10 C$ y verificar que el sistema detecte la divergencia mediante:
\begin{itemize}
    \item Aparición de NaN o Inf en las trayectorias simuladas
    \item Emisión de alerta de inestabilidad por el módulo de seguridad
\end{itemize}
\end{criterion}

\section{Optimización del Transporte (Orquestador)}

\subsection{Estabilidad del Algoritmo de Sinkhorn}

\begin{testcase}[Convergencia en Dominio Logarítmico]
Evaluar la convergencia del algoritmo de Sinkhorn en el dominio logarítmico frente a un parámetro de regularización $\varepsilon$ decreciente, asegurando que no ocurra \textit{underflow} hasta el límite de $\varepsilon \geq 10^{-4}$.
\end{testcase}

\begin{criterion}
El algoritmo de Sinkhorn-Knopp en escala logarítmica debe converger cuando:
\[
\varepsilon \geq 10^{-4}
\]
Para $\varepsilon < 10^{-4}$, el sistema debe detectar riesgo de \textit{underflow} numérico y emitir advertencia. La convergencia se mide mediante:
\[
\|K \text{diag}(u) K^T \text{diag}(v) - \mu\|_1 < 10^{-6}
\]
donde $K_{ij} = \exp(-C_{ij}/\varepsilon)$ es el kernel de Gibbs.
\end{criterion}

\subsection{Normalización del Simplex}

\begin{testcase}[Conservación de Masa Probabilística]
Confirmar que, tras cualquier actualización del flujo JKO (Jordan-Kinderlehrer-Otto), la suma de los pesos de los núcleos sea estrictamente $\sum_i \rho_i = 1.0$.
\end{testcase}

\begin{criterion}
Después de cada iteración del esquema JKO:
\[
\rho^{n+1} = \arg\min_{\rho \in \mathcal{P}(\mathcal{X})} \left\{ W_2^2(\rho, \rho^n) + \tau \mathcal{F}[\rho] \right\}
\]
se debe verificar que $\rho^{n+1}$ pertenece al simplex probabilístico:
\[
\sum_{i=1}^{N} \rho_i^{n+1} = 1.0, \quad \rho_i^{n+1} \geq 0 \quad \forall i
\]
con tolerancia numérica $|\sum_i \rho_i^{n+1} - 1.0| < 10^{-10}$.
\end{criterion}

\section{Solución de la Ecuación HJB mediante DGM (Rama B)}

La Rama B del predictor estocástico utiliza el Deep Galerkin Method (DGM) para resolver la ecuación de Hamilton-Jacobi-Bellman asociada a problemas de control óptimo estocástico. Estas pruebas validan la convergencia y estabilidad del método neuronal.

\subsection{Estabilidad de Gradiente}

\begin{testcase}[Explosión de Gradiente en Alta Volatilidad]
Monitorear la norma de los gradientes durante el entrenamiento de la PDE para detectar explosiones de gradiente en condiciones de alta volatilidad del proceso estocástico subyacente.
\end{testcase}

\begin{criterion}
La ecuación HJB en su forma general:
\[
\frac{\partial V}{\partial t} + \sup_{u \in \mathcal{U}} \left\{ \mathcal{L}^u V(x,t) + f(x,u,t) \right\} = 0
\]
se aproxima mediante una red neuronal $V_\theta(x,t)$ entrenada con el método DGM. Durante el entrenamiento, se debe monitorear:
\begin{enumerate}
    \item La norma del gradiente de la pérdida respecto a los pesos de la red:
    \[
    \|\nabla_\theta \mathcal{L}_{DGM}(\theta)\|_2 = \left\| \frac{\partial}{\partial \theta} \mathbb{E}[|PDE[V_\theta]|^2 + |BC[V_\theta]|^2] \right\|_2
    \]
    \item Aplicar gradient clipping cuando $\|\nabla_\theta \mathcal{L}\|_2 > C_{clip}$ con $C_{clip} = 10.0$
    \item En escenarios de alta volatilidad ($\sigma(x,t) > 2\sigma_0$), el sistema debe:
    \begin{itemize}
        \item Detectar si $\|\nabla_\theta \mathcal{L}\|_2$ excede $C_{clip}$ durante más de 5 iteraciones consecutivas
        \item Emitir \texttt{GradientInstabilityEvent}
        \item Reducir adaptativamente la tasa de aprendizaje: $\eta \to 0.5 \eta$
        \item Verificar estabilización dentro de las siguientes 20 épocas
    \end{itemize}
\end{enumerate}
\end{criterion}

\begin{remark}
Las explosiones de gradiente en DGM suelen originarse en la interacción entre los términos de derivadas de segundo orden (curvatura de la solución) y los coeficientes de difusión. La detección temprana y el clipping adaptativo son esenciales para la convergencia en régimenes de alta volatilidad.
\end{remark}

\subsection{Validación de Soluciones de Viscosidad}

\begin{testcase}[Principio de Comparación de Crandall-Lions]
Validar que la solución de la red neuronal $V_\theta(x,t)$ respete el principio de comparación para soluciones de viscosidad, garantizando unicidad y consistencia con la teoría de PDEs no lineales.
\end{testcase}

\begin{criterion}
El principio de comparación establece que, si $u$ es una supersolución de viscosidad y $v$ es una subsolución de viscosidad de la misma ecuación HJB con $u \geq v$ en la frontera, entonces $u \geq v$ en todo el dominio.

Para validar numéricamente este principio:
\begin{enumerate}
    \item Construir una solución de referencia $V_{ref}(x,t)$ mediante un método establecido (esquema de diferencias finitas upwind, método de líneas)
    \item Entrenar la red DGM para obtener $V_\theta(x,t)$
    \item Verificar monotonicidad en el tiempo (para problemas de horizonte finito):
    \[
    V_\theta(x, t_1) \geq V_\theta(x, t_2) \quad \forall t_1 < t_2, \, \forall x \in \mathcal{D}
    \]
    (suponiendo costos no negativos)
    \item Validar la propiedad de subsupersolución en una malla de puntos de test $\{(x_i, t_j)\}_{i,j}$:
    \begin{align*}
    \text{Subsolución:} \quad & PDE[V_\theta](x_i, t_j) \leq \epsilon_{tol} \\
    \text{Supersolución:} \quad & PDE[V_\theta](x_i, t_j) \geq -\epsilon_{tol}
    \end{align*}
    con tolerancia $\epsilon_{tol} = 10^{-3}$
    \item Comparar con solución de referencia:
    \[
    \|V_\theta - V_{ref}\|_{L^\infty(\mathcal{D})} < \delta_{viscosity}
    \]
    donde $\delta_{viscosity} = 0.05 \cdot \|V_{ref}\|_{L^\infty}$ (error relativo $< 5\%$)
\end{enumerate}
\end{criterion}

\begin{remark}
La teoría de soluciones de viscosidad de Crandall-Lions es el marco riguroso para ecuaciones HJB no diferenciables en sentido clásico. Las redes neuronales DGM, al ser funciones suaves ($C^\infty$), deben aproximar estas soluciones generalizadas. La validación mediante el principio de comparación es crítica para garantizar que la solución neuronal no exhibe artefactos espurios como oscilaciones no físicas o violaciones de causalidad.
\end{remark}

\subsection{Test de Entropía de Entrenamiento (Colapso de Modo)}

Durante el entrenamiento de redes neuronales para PDEs, existe el riesgo de que la red colapse a soluciones triviales (por ejemplo, una constante) que satisfacen formalmente la ecuación diferencial pero carecen de contenido informativo. Este test detecta y previene dicho comportamiento patológico.

\begin{testcase}[Detección de Colapso de Modo]
Verificar que la red neuronal no sufra de colapso de modo, donde predice una solución constante o degenerada que satisface trivialmente la PDE pero no captura la estructura real de la solución.
\end{testcase}

\begin{criterion}
El colapso de modo ocurre cuando la red aprende una solución de varianza mínima (típicamente constante) que minimiza la pérdida de la PDE sin resolver realmente el problema de valor en la frontera.

Para detectarlo, se debe verificar la proporcionalidad entre la varianza de la solución neuronal y la condición terminal:
\begin{enumerate}
    \item Calcular la varianza espacial de la solución en tiempo $t < T$:
    \[
    \text{Var}_x[V_\theta(x,t)] = \mathbb{E}_x[(V_\theta(x,t) - \bar{V}_t)^2]
    \]
    donde $\bar{V}_t = \mathbb{E}_x[V_\theta(x,t)]$ es el valor medio sobre el dominio espacial
    \item Calcular la varianza de la condición terminal (payoff):
    \[
    \text{Var}[g(\xi)] = \mathbb{E}_\xi[(g(\xi) - \bar{g})^2]
    \]
    donde $g(\xi)$ es la condición de frontera terminal y $\xi$ representa los estados terminales relevantes
    \item Verificar proporcionalidad:
    \[
    \kappa_{\text{low}} \leq \frac{\text{Var}_x[V_\theta(x,t)]}{\text{Var}[g(\xi)]} \leq \kappa_{\text{high}}
    \]
    con umbrales típicos: $\kappa_{\text{low}} = 0.3$ y $\kappa_{\text{high}} = 1.2$
    \item Si el ratio cae por debajo de $\kappa_{\text{low}}$: detectar colapso de modo
    \item Monitorear la entropía diferencial de la distribución de valores predichos:
    \[
    H[V_\theta] = -\int p(v) \log p(v) \, dv
    \]
    Una solución colapsada tendrá $H[V_\theta] \to -\infty$ (distribución delta)
\end{enumerate}

Criterios de aceptación:
\begin{itemize}
    \item El ratio de varianzas debe estar dentro de $[\kappa_{\text{low}}, \kappa_{\text{high}}]$ en al menos el $90\%$ de los tiempos $t \in [0,T]$
    \item La entropía diferencial debe satisfacer: $H[V_\theta] > H_{\text{min}}$ con $H_{\text{min}}$ calibrado según la complejidad del problema
    \item La norma del gradiente de $V_\theta$ respecto a $x$ no debe colapsar:
    \[
    \mathbb{E}_x[\|\nabla_x V_\theta(x,t)\|_2] > \epsilon_{\text{grad}} > 0
    \]
    (evitar soluciones planas espacialmente)
\end{itemize}
\end{criterion}

\begin{remark}
El colapso de modo es particularmente común en:
\begin{enumerate}
    \item Problemas con condiciones de frontera uniformes o simétricas
    \item Tasas de aprendizaje excesivamente altas que llevan a soluciones de mínima norma
    \item Arquitecturas de red sub-dimensionadas que no pueden capturar la complejidad de la solución verdadera
    \item Inicializaciones de pesos que sesgan hacia valores constantes
\end{enumerate}
La detección temprana mediante monitoreo de varianza permite reinicializar el entrenamiento o ajustar hiperparámetros antes de que el colapso se consolide.
\end{remark}

\begin{remark}
Desde el punto de vista de teoría del control, una solución colapsada corresponde a una política de control degenerada que no responde a las variaciones del estado. Matemáticamente, si $V_\theta(x,t) \approx c$ (constante), entonces la política óptima derivada $u^*(x,t) = \arg\max_u \{\cdots + \nabla_x V \cdot b(x,u)\}$ se vuelve indeterminada o trivial, perdiendo toda utilidad práctica.
\end{remark}

\subsection{Test de Convergencia en Refinamiento de Malla}

\begin{testcase}[Consistencia con Refinamiento]
Verificar que la solución DGM converja a la solución exacta (o de referencia) cuando se incrementa la densidad de puntos de colocación en el dominio espacio-temporal.
\end{testcase}

\begin{criterion}
Para una secuencia de mallas anidadas $\mathcal{M}_1 \subset \mathcal{M}_2 \subset \mathcal{M}_3$ con densidades $N_1 < N_2 < N_3$ de puntos de colocación:
\begin{enumerate}
    \item Entrenar DGM en cada malla hasta convergencia de la pérdida: $\mathcal{L}_{DGM}^{(k)} < 10^{-4}$
    \item Calcular el error en una malla fina de evaluación independiente:
    \[
    e_k = \|V_{\theta_k} - V_{ref}\|_{L^2(\mathcal{D}_{eval})}
    \]
    \item Verificar convergencia monótona:
    \[
    e_1 > e_2 > e_3
    \]
    \item Estimar la tasa de convergencia empírica:
    \[
    r = \frac{\log(e_1/e_2)}{\log(N_2/N_1)}
    \]
    Se espera $r \geq 0.5$ (convergencia al menos de orden $N^{-0.5}$)
\end{enumerate}
\end{criterion}

\subsection{Test Simplificado de Varianza de Salida (Mode Collapse Detection)}

Este test complementa el análisis de entropía de entrenamiento con un criterio cuantitativo directo sobre la varianza de la salida de la red neuronal, proporcionando una métrica de alerta temprana para colapso de modo.

\begin{testcase}[Umbral de Varianza Mínima]
Comparar la varianza de la salida de la red neuronal $V_\theta(x,t)$ frente a una solución de referencia $V_{ref}(x,t)$ y verificar que la red capture al menos el $10\%$ de la variabilidad de la solución verdadera.
\end{testcase}

\begin{criterion}
Sea $V_{ref}(x,t)$ una solución de referencia obtenida mediante un método establecido (diferencias finitas, elementos finitos, o solución analítica cuando esté disponible). Para la red DGM entrenada $V_\theta(x,t)$:

\begin{enumerate}
    \item Calcular la varianza de la solución de referencia sobre el dominio espacial en un instante de tiempo $t \in [0,T]$:
    \[
    \text{Var}_{ref}(t) = \frac{1}{|\mathcal{X}|} \sum_{x_i \in \mathcal{X}} (V_{ref}(x_i, t) - \bar{V}_{ref}(t))^2
    \]
    donde $\bar{V}_{ref}(t) = \frac{1}{|\mathcal{X}|} \sum_{x_i} V_{ref}(x_i, t)$ es el valor medio.
    
    \item Calcular la varianza de la salida neuronal:
    \[
    \text{Var}_{\theta}(t) = \frac{1}{|\mathcal{X}|} \sum_{x_i \in \mathcal{X}} (V_\theta(x_i, t) - \bar{V}_\theta(t))^2
    \]
    
    \item Evaluar el ratio de varianzas:
    \[
    R_{var}(t) = \frac{\text{Var}_{\theta}(t)}{\text{Var}_{ref}(t)}
    \]
    
    \item \textbf{Criterio de Fallo:} El test falla (detecta colapso de modo) si:
    \[
    R_{var}(t) < 0.10 \quad \text{para algún } t \in [0, 0.9T]
    \]
    Esto indica que la red ha colapsado a una solución de varianza mínima (típicamente constante o casi-constante) que no captura la estructura de la PDE.
    
    \item \textbf{Criterio de Aceptación:} El test pasa si:
    \[
    R_{var}(t) \geq 0.10 \quad \forall t \in [0, 0.9T]
    \]
    y además:
    \[
    \text{mediana}_{t \in [0,T]} R_{var}(t) \geq 0.50
    \]
    garantizando que la red captura al menos el $50\%$ de la variabilidad en la mayoría de los tiempos.
\end{enumerate}
\end{criterion}

\begin{remark}
Este test es especialmente útil durante el entrenamiento iterativo:
\begin{itemize}
    \item Si $R_{var}(t) < 0.10$ se detecta antes de la convergencia, se debe interrumpir el entrenamiento y ajustar hiperparámetros (tasa de aprendizaje, arquitectura de red, inicialización de pesos)
    \item El umbral del $10\%$ es conservador y refleja que cualquier solución que capture menos del $10\%$ de la variabilidad es prácticamente una constante
    \item La restricción a $t \in [0, 0.9T]$ excluye la vecindad de la condición terminal donde la varianza puede naturalmente reducirse al acercarse al payoff
\end{itemize}
\end{remark}

\begin{remark}
Este criterio es complementario al test de entropía de la subsección anterior. Mientras que el test de entropía usa el ratio $\kappa_{\text{low}} = 0.3$, este test usa un umbral más estricto ($0.10$) para detección conservadora de colapsos severos. En la práctica:
\begin{itemize}
    \item $R_{var} < 0.10$: colapso crítico (fallo inmediato)
    \item $0.10 \leq R_{var} < 0.30$: advertencia de varianza baja (requiere monitoreo)
    \item $R_{var} \geq 0.50$: operación normal
\end{itemize}
\end{remark}

\chapter{Pruebas de Robustez y Circuit Breakers}

Verifican la capacidad del sistema para protegerse ante anomalías del mercado o fallos en los datos.

\section{Gestión de Outliers y Régimen}

\subsection{Inyección de Outliers}

\begin{testcase}[Valores Atípicos Extremos]
Introducir valores $> 20\sigma$ en el flujo de entrada y verificar que el sistema descarte el dato, emita la alerta de validación y mantenga el estado inercial de los pesos.
\end{testcase}

\begin{criterion}
Dado un flujo de observaciones $\{y_t\}$ con estadísticas móviles $\mu_t$, $\sigma_t$, inyectar una observación:
\[
\tilde{y}_t = \mu_t + 20\sigma_t
\]
El sistema debe:
\begin{enumerate}
    \item Detectar que $|\tilde{y}_t - \mu_t| > \theta\sigma_t$ con $\theta = 10$
    \item Rechazar la observación y NO actualizar el meta-estado $\Xi_t$
    \item Emitir evento \texttt{OutlierDetectedEvent} con metadata del valor rechazado
    \item Mantener los pesos $\{w_i\}_{i=A}^D$ sin cambios
\end{enumerate}
\end{criterion}

\subsection{Disparo de CUSUM}

\begin{testcase}[Cambio de Régimen Estructural]
Simular un cambio de régimen (deriva estructural) y validar que el evento de cambio de régimen se emita exactamente cuando el estadístico $G_t^+$ exceda el umbral dinámico $h$.
\end{testcase}

\begin{criterion}
El detector CUSUM (Cumulative Sum) para detectar cambios en la media opera mediante:
\[
G_t^+ = \max(0, G_{t-1}^+ + (y_t - \mu_0) - k)
\]
donde $k$ es el \textit{slack} de tolerancia y $h$ es el umbral de alarma. Se debe verificar:
\begin{enumerate}
    \item $G_t^+ > h \implies$ emitir \texttt{RegimeChangedEvent}
    \item Reiniciar $G_t^+ = 0$ tras la detección
    \item La detección debe ocurrir con máximo $\tau$ observaciones de retardo desde el verdadero punto de cambio
\end{enumerate}
\end{criterion}

\section{Modo de Emergencia (Postulado de Robustez)}

\subsection{Singularidad Crítica}

\begin{testcase}[Régimen de Rugosidad Extrema]
Reducir artificialmente el exponente de Hölder por debajo de $H_{min}$ y verificar que el Orquestador fuerce automáticamente el peso $w_D \to 1.0$ y cambie la función de costo a la métrica de Huber.
\end{testcase}

\begin{criterion}
Definir un umbral crítico $H_{min}$ (típicamente $H_{min} = 0.25$). Cuando el análisis WTMM detecta:
\[
\hat{H}_t < H_{min}
\]
el sistema debe:
\begin{enumerate}
    \item Activar modo de emergencia: $w_A = w_B = w_C = 0$, $w_D = 1.0$
    \item Cambiar la función de costo del Orquestador de Wasserstein a Huber:
    \[
    C(x,y) = \begin{cases}
    \frac{1}{2}|x-y|^2 & \text{si } |x-y| \leq \delta \\
    \delta(|x-y| - \frac{\delta}{2}) & \text{si } |x-y| > \delta
    \end{cases}
    \]
    \item Emitir evento \texttt{CriticalSingularityEvent}
    \item Mantener este estado hasta que $\hat{H}_t > H_{min} + \epsilon_{histeresis}$
\end{enumerate}
\end{criterion}

\chapter{Pruebas de I/O y Persistencia}

Garantizan la continuidad operativa y la integridad del estado latente.

\section{Protocolo de Snapshots}

\subsection{Hot-Start}

\begin{testcase}[Continuidad de Estado]
Serializar el meta-estado $\Xi_t$, reiniciar el sistema y realizar una carga (Load). La primera predicción tras el reinicio debe ser idéntica a la que se habría generado sin interrupción.
\end{testcase}

\begin{criterion}
El meta-estado completo se define como:
\[
\Xi_t = \left\{ \{w_i\}_{i=A}^D, \{\theta_i^*\}_{i=A}^D, \mathcal{H}_t, \text{Sig}_t, G_t^{\pm}, \mu_t, \sigma_t^2 \right\}
\]
Procedimiento de validación:
\begin{enumerate}
    \item En tiempo $t_0$, serializar $\Xi_{t_0}$ a archivo binario
    \item Generar predicción $\hat{y}_{t_0+1}^{\text{original}}$
    \item Reiniciar el sistema (liberar memoria)
    \item Cargar $\Xi_{t_0}$ desde archivo
    \item Generar predicción $\hat{y}_{t_0+1}^{\text{restored}}$
    \item Verificar: $|\hat{y}_{t_0+1}^{\text{original}} - \hat{y}_{t_0+1}^{\text{restored}}| < 10^{-12}$
\end{enumerate}
\end{criterion}

\subsection{Validación de Checksum}

\begin{testcase}[Integridad Criptográfica]
Corromper un solo bit en el archivo binario del snapshot y verificar que el sistema rechace la carga mediante el hash SHA-256, forzando un \textit{Cold-Start}.
\end{testcase}

\begin{criterion}
Cada archivo de snapshot debe incluir un hash SHA-256 del contenido. El proceso de carga debe:
\begin{enumerate}
    \item Leer el archivo binario
    \item Calcular $H' = \text{SHA256}(\text{contenido})$
    \item Comparar con el hash almacenado $H$
    \item Si $H' \neq H$: rechazar carga, emitir \texttt{CorruptedSnapshotEvent}, inicializar en modo Cold-Start
    \item Si $H' = H$: proceder con la deserialización
\end{enumerate}
Para validar, modificar 1 bit en posición aleatoria del archivo y verificar el rechazo.
\end{criterion}

\section{Recuperación Ante Fallos de I/O}

La persistencia del estado $\Xi_t$ es crítica para la continuidad operativa. Estos tests validan la robustez del sistema ante fallos durante operaciones de escritura, lectura y almacenamiento.

\subsection{Interrupción de Escritura (Atomicidad)}

\begin{testcase}[Simulación de Caída de Energía]
Simular una caída de energía o desconexión abrupta del sistema durante la serialización del estado $\Xi_t$, validando que el sistema maneje archivos parcialmente escritos sin entrar en estados inconsistentes.
\end{testcase}

\begin{criterion}
El protocolo de snapshotting debe garantizar atomicidad mediante el patrón de escritura temporal:
\begin{enumerate}
    \item Serializar $\Xi_t$ a un archivo temporal: \texttt{snapshot\_\{timestamp\}.tmp}
    \item Calcular $H = \text{SHA256}(\Xi_t)$ y agregarlo al archivo
    \item Realizar \texttt{fsync()} para forzar escritura a disco
    \item Renombrar atómicamente: \texttt{snapshot\_\{timestamp\}.tmp} $\to$ \texttt{snapshot\_\{timestamp\}.bin}
\end{enumerate}

Test de validación:
\begin{enumerate}
    \item Iniciar proceso de snapshotting en tiempo $t_0$
    \item Interrumpir el proceso en un punto aleatorio $p \in \{0.1, 0.3, 0.5, 0.7, 0.9\}$ del progreso de escritura
    \item Simular terminación abrupta (enviar señal \texttt{SIGKILL} o cortar I/O)
    \item Reiniciar el sistema
    \item El sistema debe:
    \begin{itemize}
        \item Detectar ausencia del archivo principal \texttt{.bin} (nunca fue renombrado)
        \item Detectar presencia de archivo \texttt{.tmp} corrupto
        \item Ignorar el archivo temporal (no intentar cargarlo)
        \item Buscar el snapshot válido más reciente anterior a $t_0$
        \item Si no existe snapshot previo válido: ejecutar \texttt{ColdStartEvent}
        \item NO entrar en bucle de reinicio infinito
    \end{itemize}
\end{enumerate}

Criterio de éxito:
\[
\text{Tiempo de recuperación} < T_{recovery} = 30 \text{ segundos}
\]
Sin degradación de la funcionalidad del sistema tras la recuperación.
\end{criterion}

\begin{remark}
La atomicidad en la escritura de snapshots es fundamental para evitar el \textit{torn write problem}, donde una escritura parcial deja el archivo en un estado inconsistente que no puede detectarse mediante checksum si la interrupción ocurre antes del cálculo del hash. El patrón write-to-temp-then-rename aprovecha la garantía de atomicidad que la mayoría de sistemas de archivos modernos (ext4, XFS, NTFS, APFS) ofrecen para la operación de renombrado.
\end{remark}

\subsection{Corrupción Silenciosa de Disco}

\begin{testcase}[Bit Rot y Errores de Almacenamiento]
Detectar y manejar la corrupción silenciosa de datos en el almacenamiento persistente (bit rot, degradación de medios) que puede ocurrir entre el momento de escritura y el de lectura del snapshot.
\end{testcase}

\begin{criterion}
Aunque el snapshot se haya escrito correctamente, el medio de almacenamiento puede introducir errores posteriormente. El protocolo debe:
\begin{enumerate}
    \item Almacenar metadata de verificación junto al snapshot:
    \begin{itemize}
        \item Hash SHA-256 del contenido
        \item Timestamp de creación
        \item Versión del formato de serialización
        \item Checksum CRC32 de validación rápida (opcional, pre-check antes de SHA-256)
    \end{itemize}
    \item Durante la carga:
    \begin{itemize}
        \item Verificar CRC32 (si está disponible) como pre-filtro rápido
        \item Verificar SHA-256 completo
        \item Si falla verificación: marcar archivo como corrupto, buscar snapshot anterior
    \end{itemize}
    \item Implementar política de retención con redundancia:
    \begin{itemize}
        \item Mantener los últimos $N = 5$ snapshots válidos
        \item Permitir recuperación desde snapshot $t_{-k}$ si $t_0$ está corrupto
    \end{itemize}
\end{enumerate}

Test de validación:
\begin{enumerate}
    \item Crear snapshot válido en $t_0$
    \item Introducir corrupción en el archivo binario (modificar bytes aleatorios)
    \item Intentar cargar el snapshot
    \item Verificar que el sistema:
    \begin{itemize}
        \item Detecte la corrupción mediante checksum
        \item Emita \texttt{CorruptedSnapshotEvent} con metadata del archivo afectado
        \item Busque snapshot alternativo en la cola de retención
        \item Si encuentra snapshot válido anterior: cargarlo (con advertencia de pérdida de estado parcial)
        \item Si no existe snapshot válido: ejecutar Cold-Start
        \item NO entre en bucle infinito de verificación-fallo-reinicio
    \end{itemize}
\end{enumerate}
\end{criterion}

\subsection{Agotamiento de Espacio en Disco}

\begin{testcase}[Manejo de Capacidad Insuficiente]
Validar el comportamiento del sistema cuando el dispositivo de almacenamiento se queda sin espacio durante la escritura de un snapshot.
\end{testcase}

\begin{criterion}
Durante la operación de snapshotting, el sistema debe:
\begin{enumerate}
    \item Antes de iniciar escritura: verificar espacio disponible
    \[
    \text{Espacio\_libre} \geq 2 \times \text{Tamaño\_estimado}(\Xi_t)
    \]
    (factor de 2 por seguridad para escritura temporal + renombrado)
    \item Si no hay espacio suficiente:
    \begin{itemize}
        \item Emitir \texttt{InsufficientStorageEvent}
        \item NO intentar la escritura
        \item Mantener el snapshot anterior válido
        \item Continuar operación en memoria (sin persistencia) hasta que se libere espacio
    \end{itemize}
    \item Si el error de espacio ocurre durante la escritura (fallo de estimación):
    \begin{itemize}
        \item Capturar excepción de I/O
        \item Eliminar archivo temporal corrupto
        \item Emitir \texttt{SnapshotWriteFailedEvent}
        \item NO corromper el snapshot anterior válido
    \end{itemize}
\end{enumerate}

Test de validación:
\begin{enumerate}
    \item Crear un volumen de almacenamiento con capacidad limitada
    \item Llenar el volumen artificialmente dejando espacio insuficiente
    \item Intentar crear snapshot
    \item Verificar detección preventiva y manejo graceful sin crash
\end{enumerate}
\end{criterion}

\begin{remark}
El agotamiento de espacio en disco es un escenario común en sistemas de producción. El diseño debe priorizar: (1) no corromper snapshots existentes válidos, (2) degradar gracefully manteniendo operación en memoria, (3) proveer telemetría clara para intervención del operador, (4) recuperar automáticamente cuando se libere espacio.
\end{remark}

\chapter{Pruebas de Paridad y Fidelidad de Hardware (Cross-Platform)}

Dado que el sistema contempla implementaciones heterogéneas (CPU, GPU y FPGA), es vital añadir tests de consistencia de bit para garantizar que las diferentes arquitecturas de hardware produzcan resultados matemáticamente equivalentes.

\section{Tests de Consistencia de Bit}

\begin{testcase}[Equivalencia Multi-Arquitectura]
Verificar que los resultados numéricos de los algoritmos críticos sean consistentes entre diferentes plataformas de hardware (CPU, GPU, FPGA), dentro de los límites de precisión inherentes a cada arquitectura.
\end{testcase}

\begin{criterion}
Para cada componente crítico del sistema (generación de variables aleatorias, cálculo de signaturas, integración SDE), ejecutar el mismo conjunto de datos de entrada en:
\begin{enumerate}
    \item CPU con aritmética de punto flotante IEEE 754 (64 bits)
    \item GPU con aritmética de punto flotante (32 o 64 bits según disponibilidad)
    \item FPGA con aritmética de punto fijo (precisión configurable)
\end{enumerate}
La diferencia relativa entre implementaciones debe satisfacer:
\[
\frac{\|x^{CPU} - x^{GPU}\|_2}{\|x^{CPU}\|_2} < \epsilon_{GPU}, \quad \frac{\|x^{CPU} - x^{FPGA}\|_2}{\|x^{CPU}\|_2} < \epsilon_{FPGA}
\]
donde $\epsilon_{GPU} = 10^{-6}$ para aritmética de 32 bits y $\epsilon_{FPGA}$ es el error de cuantización del hardware de menor precisión.
\end{criterion}

\section{Test de Deriva Numérica}

\subsection{Rama D: Signatures en FPGA vs. CPU}

\begin{testcase}[Acumulación de Error en Punto Fijo]
Comparar los resultados de la Rama D (cálculo de signaturas de caminos rugosos) ejecutada en FPGA con aritmética de punto fijo frente a la implementación en CPU con punto flotante de 64 bits.
\end{testcase}

\begin{criterion}[Criterio de Aceptación de Deriva]
La signatura de un camino $\gamma: [0,T] \to \mathbb{R}^d$ se calcula mediante la expansión truncada:
\[
\text{Sig}^{(M)}(\gamma) = \left(1, \int_0^T d\gamma_{t_1}, \int_0^T \int_0^{t_1} d\gamma_{t_2} \otimes d\gamma_{t_1}, \ldots \right)
\]
Para un test de 10,000 iteraciones (actualizaciones secuenciales de la signatura), se debe cumplir:
\begin{enumerate}
    \item Ejecutar el algoritmo de signatura en CPU (double precision): $\text{Sig}^{CPU}_{10000}$
    \item Ejecutar el mismo algoritmo en FPGA (punto fijo, $n$ bits): $\text{Sig}^{FPGA}_{10000}$
    \item Calcular la divergencia acumulada en norma infinito:
    \[
    \Delta_{acum} = \|\text{Sig}^{CPU}_{10000} - \text{Sig}^{FPGA}_{10000}\|_{\infty}
    \]
    \item \textbf{Criterio Primario (Cota de Error):} Verificar que:
    \[
    \Delta_{acum} \leq N \cdot \epsilon_{quant}
    \]
    donde $N = 10000$ es el número de iteraciones y $\epsilon_{quant} = 2^{-n+1}$ es el error de cuantización de la FPGA de $n$ bits de precisión.
    
    \item \textbf{Criterio Secundario (Preservación de Propiedades Topológicas):} Para asegurar que el error de cuantización no invalida la firma topológica, se deben verificar adicionalmente:
    
    \begin{itemize}
        \item \textbf{Norma de la Signatura:} El error relativo en la norma debe ser acotado:
        \[
        \frac{|\|\text{Sig}^{FPGA}_{10000}\|_2 - \|\text{Sig}^{CPU}_{10000}\|_2|}{\|\text{Sig}^{CPU}_{10000}\|_2} < \tau_{norm}
        \]
        con $\tau_{norm} = 0.01$ (error relativo $< 1\%$). Esto garantiza que la magnitud global de la firma se preserva.
        
        \item \textbf{Preservación de Orden de Componentes:} La estructura jerárquica de las componentes tensoriales debe mantenerse. Sea $s_i^{(k)}$ la $i$-ésima componente del nivel $k$ de la signatura. Verificar que:
        \[
        \text{sgn}(s_i^{(k),CPU}) = \text{sgn}(s_i^{(k),FPGA}) \quad \forall i,k
        \]
        garantizando que no hay inversiones de signo que indiquen corrupción topológica.
        
        \item \textbf{Distancia Angular (Cosine Similarity):} Medir la similitud angular entre las firmas:
        \[
        \cos(\theta) = \frac{\langle \text{Sig}^{CPU}_{10000}, \text{Sig}^{FPGA}_{10000} \rangle}{\|\text{Sig}^{CPU}_{10000}\|_2 \cdot \|\text{Sig}^{FPGA}_{10000}\|_2}
        \]
        Se requiere $\cos(\theta) > 0.9999$ (desviación angular $< 0.81^\circ$), asegurando que las firmas apuntan esencialmente en la misma dirección en el espacio de firmas.
        
        \item \textbf{Error Relativo por Nivel:} Para cada nivel $k$ de la expansión tensorial:
        \[
        \frac{\|\text{Sig}^{CPU,(k)}_{10000} - \text{Sig}^{FPGA,(k)}_{10000}\|_2}{\|\text{Sig}^{CPU,(k)}_{10000}\|_2} < \tau_k
        \]
        donde $\tau_k = 0.05 \cdot k$ permite más tolerancia en niveles superiores pero mantiene estricta concordancia en los primeros niveles (que capturan la geometría macroscópica del camino).
    \end{itemize}
    
    \item \textbf{Criterio de Fallo:} El test falla si:
    \begin{itemize}
        \item $\Delta_{acum} > N \cdot \epsilon_{quant}$ (violación del criterio primario), o
        \item Cualquiera de los criterios de preservación topológica es violado
    \end{itemize}
    indicando que el error de cuantización acumulado ha degradado la integridad semántica de la firma topológica.
\end{enumerate}
\end{criterion}

\begin{remark}
La deriva numérica en sistemas de punto fijo es inevitable debido al truncamiento repetido. El criterio primario establece que la acumulación de error no debe superar el producto del número de iteraciones por el quantum de representación, lo cual representa el peor caso teórico bajo la hipótesis de errores independientes.

Sin embargo, para la Rama D, no basta con que el error sea acotado en norma; es esencial que las \textit{propiedades cualitativas} de la signatura se preserven:
\begin{itemize}
    \item Las firmas de caminos se usan para discriminar trayectorias en espacios de alta dimensión
    \item Pequeñas corrupciones angulares pueden llevar a clasificaciones erróneas
    \item La preservación de signos en las componentes es crítica porque reflejan la orientación de bucles y excursiones del camino
    \item El error relativo por nivel asegura que los niveles bajos (momento, área orientada) que dominan la geometría macroscópica tengan precisión extrema
\end{itemize}
\end{remark}

\begin{remark}
Para una FPGA con $n = 32$ bits (punto fijo Q16.16), el error de cuantización es $\epsilon_{quant} = 2^{-15} \approx 3.05 \times 10^{-5}$. Tras 10,000 iteraciones, el criterio primario permite $\Delta_{acum} \leq 0.305$, pero los criterios topológicos exigen:
\begin{itemize}
    \item Error relativo en norma $< 1\%$
    \item Desviación angular $< 1^\circ$
    \item Error en primer nivel $< 5\%$
\end{itemize}
Estos criterios adicionales garantizan integridad funcional más allá de la mera acotación numérica.
\end{remark}

\subsection{Reproducibilidad Determinista}

\begin{testcase}[Inicialización Controlada de Semillas]
Garantizar que, dada la misma semilla de generador pseudo-aleatorio, todas las plataformas (CPU, GPU, FPGA) produzcan exactamente la misma secuencia de estados.
\end{testcase}

\begin{criterion}
Fijar una semilla determinista $s_0$ para el generador de números aleatorios. Ejecutar 1,000 pasos de simulación en cada plataforma y verificar:
\[
\{X_t^{CPU}\}_{t=1}^{1000} = \{X_t^{GPU}\}_{t=1}^{1000} = \{X_t^{FPGA}\}_{t=1}^{1000}
\]
La igualdad debe ser bit-a-bit para las plataformas que comparten el mismo formato de representación (CPU y GPU en punto flotante). Para FPGA, la comparación debe hacerse tras la conversión del formato de punto fijo a punto flotante.
\end{criterion}

\section{Validación de Latencia y Throughput}

\begin{testcase}[Benchmark de Rendimiento Cross-Platform]
Medir el tiempo de ejecución y el throughput de cada rama del predictor en las tres arquitecturas para identificar cuellos de botella y validar las ventajas de la implementación heterogénea.
\end{testcase}

\begin{criterion}
Para un lote de $N = 1000$ predicciones:
\begin{align*}
T_{CPU} &= \text{tiempo total en CPU} \\
T_{GPU} &= \text{tiempo total en GPU} \\
T_{FPGA} &= \text{tiempo total en FPGA}
\end{align*}
Se espera que:
\begin{itemize}
    \item GPU supere a CPU en operaciones masivamente paralelas (integración Monte Carlo): $T_{GPU} < 0.3 \cdot T_{CPU}$
    \item FPGA supere a CPU en operaciones deterministas de baja latencia (cálculo de signaturas): $T_{FPGA} < 0.1 \cdot T_{CPU}$
\end{itemize}
El throughput se mide como:
\[
\text{Throughput} = \frac{N}{T} \quad \text{[predicciones/segundo]}
\]
\end{criterion}

\chapter{Protocolo de Validación Final (Causalidad)}

Este es el test definitivo de rendimiento predictivo que debe aplicarse antes de cualquier despliegue.

\section{Generalización}

\begin{testcase}[Rolling Walk-Forward]
Ausencia total de \textit{look-ahead bias}. El entrenamiento solo utiliza datos estrictamente anteriores al horizonte de test.
\end{testcase}

\begin{criterion}
Dividir el conjunto de datos en ventanas móviles:
\[
\mathcal{D} = \{(t_1, y_1), \ldots, (t_N, y_N)\}
\]
Para cada ventana de test $\mathcal{T}_k = \{t_{n_k}, \ldots, t_{n_k+w}\}$:
\begin{enumerate}
    \item Entrenar solo con $\mathcal{D}_{\text{train}}^k = \{(t_i, y_i) : t_i < t_{n_k}\}$
    \item Predecir en $\mathcal{T}_k$
    \item Avanzar la ventana: $k \to k+1$
    \item Acumular métricas out-of-sample (RMSE, MAE, Sharpe ratio)
\end{enumerate}
El sistema debe garantizar que en ningún momento se utilicen datos futuros para entrenar el modelo en tiempo $t$.
\end{criterion}

\section{Eficiencia de Meta-Optimización}

\begin{testcase}[Optimización Bayesiana]
Mejora iterativa del \textit{Expected Improvement} sobre el error de generalización en comparación con una búsqueda aleatoria.
\end{testcase}

\begin{criterion}
Utilizar un proceso Gaussiano (GP) como surrogate model del espacio de hiperparámetros $\Theta = \{\alpha, \beta, \gamma, \ldots\}$. Para $n$ iteraciones de optimización:
\begin{enumerate}
    \item Búsqueda aleatoria: $\theta_i \sim \text{Uniforme}(\Theta)$
    \item Optimización Bayesiana: $\theta_i = \arg\max_{\theta} \text{EI}(\theta | \mathcal{D}_{1:i-1})$
\end{enumerate}
Criterio de aceptación:
\[
\min_{i \leq n} \mathcal{L}(\theta_i^{\text{BO}}) < \min_{i \leq n} \mathcal{L}(\theta_i^{\text{random}})
\]
donde $\mathcal{L}$ es la función de pérdida en el conjunto de validación. La probabilidad de mejora debe ser significativa (p-value $< 0.05$ en test de Mann-Whitney).
\end{criterion}

\section{Integridad Temporal}

\begin{testcase}[Métrica de Abandono (TTL)]
Cancelación de la actualización JKO si el retraso de la señal $y_{\text{target}}$ excede $\Delta_{\text{max}}$.
\end{testcase}

\begin{criterion}
Definir un \textit{Time-To-Live} (TTL) para las señales objetivo:
\[
\text{TTL}(y_t) = t_{\text{current}} - t
\]
Si una señal de entrenamiento/actualización satisface:
\[
\text{TTL}(y_t) > \Delta_{\text{max}}
\]
el sistema debe:
\begin{enumerate}
    \item Descartar la señal
    \item NO ejecutar la actualización del Orquestador
    \item Emitir \texttt{StaleDateEvent}
    \item Registrar en log la marca temporal de la señal rechazada
\end{enumerate}
Valor típico: $\Delta_{\text{max}} = 5$ segundos en sistemas de alta frecuencia.
\end{criterion}

\subsection{Test de Inyección de Lag}

La política de abandono es crucial para evitar operar con "pesos obsoletos" que reflejan un estado del mercado ya irrelevante. Este test valida el comportamiento del sistema bajo condiciones de latencia extrema.

\begin{testcase}[Retraso Artificial de Señal]
Retrasar artificialmente la señal $y_{\text{target}}$ por encima de $\Delta_{\text{max}}$ y validar que el sistema active modo de inferencia degradada y suspenda el transporte óptimo.
\end{testcase}

\begin{criterion}
Procedimiento de validación:
\begin{enumerate}
    \item Configurar el sistema con $\Delta_{\text{max}} = 5$ segundos
    \item Generar un flujo de datos en tiempo real con timestamps correctos
    \item Inyectar artificialmente una señal $\tilde{y}_t$ con timestamp retrasado:
    \[
    \text{TTL}(\tilde{y}_t) = t_{\text{current}} - t = \Delta_{\text{max}} + \delta
    \]
    donde $\delta > 0$ (típicamente $\delta = 1$ segundo para hacer el test inequívoco)
    \item Verificar que el sistema ejecute la siguiente secuencia:
    \begin{itemize}
        \item Detectar $\text{TTL}(\tilde{y}_t) > \Delta_{\text{max}}$
        \item Activar bandera \texttt{DegradedInferenceMode} = \texttt{True}
        \item Suspender inmediatamente el algoritmo de transporte JKO:
        \[
        \text{JKO\_update}(\rho^{n+1}) \to \text{SUSPENDED}
        \]
        \item Congelar pesos del Orquestador en su último valor válido: $\{w_i\}_{frozen}$
        \item Emitir eventos:
        \begin{itemize}
            \item \texttt{StaleDataEvent} con metadata: $(t, t_{\text{current}}, \text{TTL})$
            \item \texttt{DegradedInferenceModeActivated}
        \end{itemize}
        \item Registrar en telemetría el tiempo de lag para análisis post-mortem
        \item Continuar generando predicciones usando solo los pesos congelados (sin actualizaciones dinámicas)
    \end{itemize}
    \item Verificar condición de recuperación: cuando el flujo de datos se normalice y aparezcan señales con $\text{TTL}(y_t) < 0.8 \cdot \Delta_{\text{max}}$ (umbral de histéresis), el sistema debe:
    \begin{itemize}
        \item Desactivar \texttt{DegradedInferenceMode}
        \item Reanudar transporte JKO
        \item Emitir \texttt{NormalOperationRestoredEvent}
    \end{itemize}
\end{enumerate}

Criterios de aceptación:
\begin{itemize}
    \item Tiempo de detección: $< 100$ milisegundos desde la recepción de $\tilde{y}_t$
    \item NO se debe ejecutar ni una sola iteración de JKO con datos obsoletos
    \item El sistema NO debe crashear ni entrar en estado indefinido
    \item Las predicciones deben continuar (aunque degradadas) con última configuración válida
\end{itemize}
\end{criterion}

\begin{remark}
El modo de inferencia degradada es un mecanismo de defensa crítico en sistemas de trading de alta frecuencia. Operar con pesos optimizados en base a información obsoleta es matemáticamente equivalente a resolver el problema de optimización incorrecto, lo cual puede llevar a pérdidas significativas. El sistema prioriza correctitud sobre completitud: es preferible operar con pesos estáticos pero consistentes que con pesos "óptimos" calculados sobre datos irrelevantes.
\end{remark}

\chapter{Casos Extremos (Edge Cases) y Límites Operacionales}

Este capítulo documenta escenarios límite que validan el comportamiento del sistema en condiciones de frontera teóricas y operacionales.

\section{CUSUM: Umbral Dinámico Adaptativo}

\begin{testcase}[Adaptación a Regímenes de Volatilidad]
Validar que el umbral del detector CUSUM se adapte correctamente a regímenes de baja y alta volatilidad mediante la formulación dinámica $h = k \cdot \sigma_{\text{resid}}$.
\end{testcase}

\begin{criterion}
El detector CUSUM opera con umbral dinámico:
\[
h_t = k \cdot \sigma_{\text{resid}, t}
\]
donde $k \in [3, 5]$ es un factor de sensibilidad y $\sigma_{\text{resid}, t}$ es la desviación estándar móvil del residuo de predicción.

Test de validación:
\begin{enumerate}
    \item \textbf{Régimen de baja volatilidad:}
    \begin{itemize}
        \item Generar una señal con $\sigma_{\text{true}} = 0.01$
        \item Estimar $\hat{\sigma}_{\text{resid}} \approx 0.01$
        \item Verificar $h_t = k \cdot 0.01$ (umbral bajo)
        \item Inyectar un cambio de deriva pequeño: $\Delta \mu = 0.05$
        \item El detector debe activarse cuando $G_t^+ > h_t$ con sensibilidad alta
    \end{itemize}
    \item \textbf{Régimen de alta volatilidad:}
    \begin{itemize}
        \item Generar una señal con $\sigma_{\text{true}} = 0.50$ (volatilidad 50× mayor)
        \item Estimar $\hat{\sigma}_{\text{resid}} \approx 0.50$
        \item Verificar $h_t = k \cdot 0.50$ (umbral alto)
        \item Inyectar el mismo cambio: $\Delta \mu = 0.05$
        \item El detector NO debe activarse (el cambio está dentro del ruido)
        \item Inyectar cambio mayor: $\Delta \mu = 2.0$
        \item Ahora sí debe detectar el cambio de régimen
    \end{itemize}
    \item \textbf{Transición de regímenes:}
    \begin{itemize}
        \item Simular una transición de baja a alta volatilidad
        \item Verificar que $h_t$ se actualice suavemente (ventana móvil)
        \item No debe haber activaciones espurias durante la transición
    \end{itemize}
\end{enumerate}

Criterio de aceptación:
\[
\frac{h_{\text{high-vol}}}{h_{\text{low-vol}}} = \frac{\sigma_{\text{high}}}{\sigma_{\text{low}}} \pm 0.1
\]
El ratio de umbrales debe seguir el ratio de volatilidades con tolerancia del 10\%.
\end{criterion}

\section{Orquestador: Convergencia a Máxima Entropía}

\begin{testcase}[Pesos Uniformes ante Incertidumbre Total]
Confirmar que el sistema converge a pesos uniformes $w = [0.25, 0.25, 0.25, 0.25]$ cuando el parámetro de regularización de Sinkhorn tiende a infinito ($\varepsilon \to \infty$), representando máxima incertidumbre.
\end{testcase}

\begin{criterion}
En el algoritmo de Sinkhorn, el parámetro de entropía $\varepsilon$ controla la regularización:
\[
\min_{\pi \in \Pi(\mu, \nu)} \left\{ \langle C, \pi \rangle - \varepsilon H(\pi) \right\}
\]
donde $H(\pi) = -\sum_{ij} \pi_{ij} \log \pi_{ij}$ es la entropía de Shannon.

Comportamiento límite:
\begin{enumerate}
    \item Cuando $\varepsilon \to 0$: La solución converge al transporte óptimo de Monge-Kantorovich (determinista)
    \item Cuando $\varepsilon \to \infty$: La entropía domina, forzando máxima dispersión
\end{enumerate}

Test de validación:
\begin{enumerate}
    \item Configurar el Orquestador con cuatro ramas activas $(A, B, C, D)$
    \item Ejecutar optimización con $\varepsilon_k = 10^k$ para $k \in \{0, 1, 2, 3, 4\}$
    \item Para cada $\varepsilon_k$, registrar los pesos resultantes $\{w_A^k, w_B^k, w_C^k, w_D^k\}$
    \item Verificar convergencia a uniformidad:
    \[
    \lim_{\varepsilon \to \infty} \{w_i\} = \left\{\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}\right\}
    \]
    \item Criterio numérico para $\varepsilon = 10^4$:
    \[
    \max_{i \in \{A,B,C,D\}} |w_i - 0.25| < 0.01
    \]
    (todos los pesos dentro del 1\% del valor uniforme)
\end{enumerate}

Interpretación:
\begin{itemize}
    \item $\varepsilon$ alto $\implies$ el sistema no puede distinguir entre ramas $\implies$ pesos uniformes por máxima entropía
    \item Es el principio de máxima entropía de Jaynes: ante ausencia de información, la distribución uniforme maximiza la entropía
\end{itemize}
\end{criterion}

\begin{remark}
Este test valida que el Orquestador respeta principios fundamentales de teoría de la información. En condiciones de incertidumbre total (todas las ramas son indistinguibles), el sistema debe evitar favorecer arbitrariamente una rama sobre otra, distribuyendo uniformemente los pesos.
\end{remark}

\section{Rama D: Invariancia ante Reparametrización Temporal}

\begin{testcase}[Invariancia de Signatures a Time-Warping]
Probar el sistema con una señal y su versión "estirada" temporalmente; la signatura del camino rugoso debe ser idéntica bajo reparametrizaciones estrictamente crecientes.
\end{testcase}

\begin{criterion}
Una propiedad fundamental de las signaturas de caminos rugosos es su invariancia ante reparametrizaciones:
\[
\text{Sig}(\gamma) = \text{Sig}(\gamma \circ \phi)
\]
para cualquier función estrictamente creciente $\phi: [0,1] \to [0,1]$ con $\phi(0) = 0$, $\phi(1) = 1$.

Test de validación:
\begin{enumerate}
    \item Generar una señal de referencia $\gamma(t)$ para $t \in [0, 1]$
    \item Calcular su signatura truncada: $S_0 = \text{Sig}^{(M)}(\gamma)$
    \item Aplicar reparametrización no lineal:
    \[
    \phi_1(t) = t^2 \quad (\text{aceleración})
    \]
    \[
    \phi_2(t) = \sqrt{t} \quad (\text{desaceleración})
    \]
    \[
    \phi_3(t) = \frac{1}{2}(1 - \cos(\pi t)) \quad (\text{sigmoidal})
    \]
    \item Para cada reparametrización, computar:
    \[
    \gamma_i(t) = \gamma(\phi_i(t))
    \]
    y su signatura:
    \[
    S_i = \text{Sig}^{(M)}(\gamma_i)
    \]
    \item Verificar invariancia:
    \[
    \|S_i - S_0\|_2 < \epsilon_{\text{inv}} \quad \forall i \in \{1, 2, 3\}
    \]
    con tolerancia $\epsilon_{\text{inv}} = 10^{-8}$
\end{enumerate}

Test de no-invariancia (control negativo):
\begin{enumerate}
    \item Aplicar una reparametrización NO monótona: $\psi(t) = t^2 - 0.5t$ (tiene un mínimo local)
    \item Verificar que $\text{Sig}(\gamma \circ \psi) \neq \text{Sig}(\gamma)$
    \item Este test confirma que el algoritmo no es trivialmente invariante a TODA transformación
\end{enumerate}
\end{criterion}

\begin{remark}
La invariancia a reparametrizaciones es la razón por la cual las signaturas son ideales para caracterizar la forma geométrica de trayectorias independientemente de su velocidad de ejecución. En contextos financieros, esto significa que la signatura captura el patrón intrínseco de un movimiento de precio, sin importar si ocurrió en 1 minuto o en 1 hora. Esta propiedad es crucial para la generalización temporal del predictor.
\end{remark}

\section{Tabla Resumen de Edge Cases}

\begin{table}[h]
\centering
\caption{Escenarios Límite y Casos Extremos}
\begin{tabular}{@{}llp{6cm}@{}}
\toprule
\textbf{Módulo} & \textbf{Escenario de Test} & \textbf{Propósito} \\ \midrule
CUSUM & Umbral Dinámico & Validar que el umbral basado en \\
 & $h = k \cdot \sigma_{\text{resid}}$ & $\sigma_{\text{resid}}$ se adapte correctamente \\
 & & a regímenes de baja/alta volatilidad \\[0.5em]
Orquestador & Máxima Entropía & Confirmar que el sistema converge a \\
 & ($\varepsilon \to \infty$) & pesos uniformes $[0.25, 0.25, 0.25, 0.25]$ \\
 & & ante incertidumbre total \\[0.5em]
Rama D & Invariancia ante & Probar con señal y su versión \\
 & Reparametrización & "estirada" en tiempo; Signature \\
 & & debe ser idéntica ($< 10^{-8}$ error) \\ \bottomrule
\end{tabular}
\end{table}

\chapter{Resumen de Criterios de Aceptación}

\begin{table}[h]
\centering
\caption{Tabla de Validación Global}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Test} & \textbf{Método} & \textbf{Criterio de Aceptación} \\ \midrule
Generalización & Rolling Walk-Forward & Ausencia total de look-ahead bias \\
 & & El entrenamiento usa datos $t < t_{\text{test}}$ \\[0.5em]
Eficiencia de & Optimización Bayesiana & Mejora iterativa del Expected \\
Meta-Optimización & (GP) & Improvement sobre búsqueda aleatoria \\[0.5em]
Integridad Temporal & Métrica de Abandono & Cancelación de actualización JKO \\
 & (TTL básico) & si TTL$(y) > \Delta_{\text{max}}$ \\[0.5em]
Inyección de Lag & Retraso Artificial $> \Delta_{\text{max}}$ & Activación inmediata de modo degradado \\
(TTL avanzado) & & y suspensión de transporte JKO \\[0.5em]
Nyquist Soft-Limit & Aliasing Multifractal & Congelación de Rama Topológica \\
 & (SIA) & antes de que error en $H$ supere 10\% \\[0.5em]
Estabilidad HJB-DGM & Monitoreo de Gradientes & Gradient clipping y reducción adaptativa \\
(Rama B) & en Alta Volatilidad & de $\eta$ ante explosiones sostenidas \\[0.5em]
Soluciones de & Principio de Comparación & Error $< 5\%$ vs. solución de referencia \\
Viscosidad & (Crandall-Lions) & con verificación de subsupersolución \\[0.5em]
Colapso de Modo & Entropía de Entrenamiento & Ratio de varianzas en $[0.3, 1.2]$ y \\
(DGM Rama B) & & $\text{Var}_x[V_\theta] / \text{Var}[g(\xi)]$ proporcional \\[0.5em]
Atomicidad I/O & Interrupción de Escritura & Cold-Start seguro sin bucles infinitos \\
 & (Snapshot) & y tiempo de recuperación $< 30$ seg \\[0.5em]
Corrupción Silenciosa & Detección de Bit Rot & Verificación SHA-256 con fallback a \\
 & & snapshot anterior válido \\[0.5em]
Umbral CUSUM & Adaptación a Volatilidad & Ratio de umbrales igual a ratio de \\
Dinámico & $h = k \cdot \sigma_{\text{resid}}$ & volatilidades $\pm 10\%$ \\[0.5em]
Máxima Entropía & Sinkhorn con $\varepsilon \to \infty$ & Convergencia a pesos uniformes \\
(Orquestador) & & $\max_i |w_i - 0.25| < 0.01$ \\[0.5em]
Invariancia Temporal & Reparametrización de & Error de signatura $< 10^{-8}$ entre \\
(Rama D) & Caminos Rugosos & señal original y time-warped \\[0.5em]
Consistencia de Bit & Tests Cross-Platform & Equivalencia CPU/GPU/FPGA dentro \\
 & (CPU/GPU/FPGA) & de márgenes de cuantización \\[0.5em]
Deriva Numérica & Rama D en Punto Fijo & Divergencia acumulada $\leq N \cdot \epsilon_{quant}$ \\
 & vs. Punto Flotante & tras 10,000 iteraciones \\ \bottomrule
\end{tabular}
\end{table}

\section{Consideraciones Finales}

Todos los protocolos aquí descritos son independientes del lenguaje de implementación y se basan exclusivamente en los fundamentos matemáticos y algorítmicos del Predictor Estocástico Universal.

\begin{remark}
La batería completa de pruebas debe ejecutarse antes de cada despliegue a producción. Los resultados deben documentarse en un reporte de validación que incluya:
\begin{itemize}
    \item Resumen ejecutivo de todos los tests ejecutados
    \item Métricas numéricas de cada criterio
    \item Gráficos de convergencia y distribuciones
    \item Análisis de casos de fallo (si los hubiere)
    \item Recomendaciones de calibración de parámetros
\end{itemize}
\end{remark}

\begin{remark}
Este protocolo es un documento vivo que debe evolucionar junto con el sistema. Cualquier modificación a la arquitectura teórica debe reflejarse en nuevos casos de prueba o actualización de los existentes.
\end{remark}

\end{document}
