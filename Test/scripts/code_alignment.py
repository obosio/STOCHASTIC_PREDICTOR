#!/usr/bin/env python3
"""Policy compliance checker - policies defined in code.

Validation Scope: Entire repository (policies apply repo-wide)

NOTE: CODE_AUDIT_POLICIES_SPECIFICATION.md is documentation only.
All policies are defined in code (policy_checks() function).
If policies change, update policy_checks() directly in this script.

Cache System:
- Uses scope_discovery.py to detect changed files
- Only validates modified files by default (fast incremental checks)
- Use --force-all flag for complete validation (CI/CD, pre-release)

Outputs:
- Console summary (PASS/FAIL per policy)
- JSON report: Test/results/code_alignment_last.json

Markdown is generated by Test/framework/reports.py.
"""

from __future__ import annotations

import argparse
import json
import os
import re
import sys
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Callable, Dict, List, Tuple

# Dynamic scope discovery with cache support
try:
    from Test.scripts.scope_discovery import (
        discover_changed_files,
        discover_module_files,
        discover_modules,
        get_cache_info,
        get_root,
    )

    CACHE_AVAILABLE = True
except ImportError:
    CACHE_AVAILABLE = False

    # Fallback if module not found
    def discover_modules(root: str | Path | None = None) -> List[str]:
        if root is None:
            root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
        root = Path(root) if isinstance(root, str) else root
        python_dir = root / "Python"
        if not python_dir.is_dir():
            return []
        return sorted([d.name for d in python_dir.iterdir() if d.is_dir() and (d / "__init__.py").is_file()])

    def discover_module_files(module_name: str, root: str | Path | None = None) -> List[str]:
        if root is None:
            root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
        root = Path(root) if isinstance(root, str) else root
        module_dir = root / "Python" / module_name
        if not module_dir.is_dir():
            return []
        return sorted([f.name for f in module_dir.glob("*.py")])

    def get_root() -> Path:
        return Path(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

    def discover_changed_files(root: str | Path | None = None, force_all: bool = False) -> List[str]:
        # Fallback: return all files
        root = get_root() if root is None else Path(root)
        return [str(f) for f in (root / "Python").rglob("*.py") if "__pycache__" not in str(f)]

    def get_cache_info() -> Dict:
        return {"cached_files": 0, "exists": False}


ROOT = str(get_root())
RESULTS_DIR = os.path.join(ROOT, "Test", "results")

# Auto-discovered modules (updates dynamically)
DISCOVERED_MODULES = discover_modules(ROOT)

# Global flag for cache usage (set by CLI args)
USE_CACHE = True
CHANGED_FILES_CACHE: List[str] | None = None


@dataclass(frozen=True)
class PolicyResult:
    policy_id: int
    name: str
    passed: bool
    details: str


def read_text(path: str) -> str:
    if not os.path.exists(path):
        return ""
    with open(path, "r", encoding="utf-8") as handle:
        return handle.read()


def file_exists(path: str) -> bool:
    return os.path.exists(path)


def find_in_file(pattern: str, path: str) -> bool:
    return re.search(pattern, read_text(path), re.MULTILINE) is not None


def find_in_dir(pattern: str, dir_path: str, extensions: Tuple[str, ...] = (".py",)) -> Tuple[bool, str]:
    """Search for pattern in directory.  Uses cache to only search changed files when possible."""
    global CHANGED_FILES_CACHE
    regex = re.compile(pattern, re.MULTILINE)

    # Get files to search (use cache if enabled and available)
    if USE_CACHE and CACHE_AVAILABLE and CHANGED_FILES_CACHE is not None:
        # Filter cached changed files to only those in dir_path with matching extensions
        files_to_search = [
            f for f in CHANGED_FILES_CACHE if f.startswith(dir_path) and any(f.endswith(ext) for ext in extensions)
        ]

        # Search only changed files
        for full_path in files_to_search:
            if regex.search(read_text(full_path)):
                return True, "OK"

        # Not found in changed files - check if there are unchanged files we should warn about
        all_files = []
        for root, _, files in os.walk(dir_path):
            for name in files:
                if name.endswith(extensions):
                    all_files.append(os.path.join(root, name))

        if files_to_search:  # Had changed files but pattern not found
            return (
                False,
                f"Pattern '{pattern}' not found in {len(files_to_search)} changed file(s) in {dir_path}",
            )
        else:  # No changed files in this directory
            return (
                False,
                f"Pattern '{pattern}' - no changed files in {dir_path} (skipped {len(all_files)} unchanged)",
            )

    # Fallback: search all files (when cache disabled or unavailable)
    for root, _, files in os.walk(dir_path):
        for name in files:
            if not name.endswith(extensions):
                continue
            full_path = os.path.join(root, name)
            if regex.search(read_text(full_path)):
                return True, "OK"
    return False, f"Pattern '{pattern}' not found in {dir_path}"


def check_dir_exists(dir_path: str) -> Tuple[bool, str]:
    """Check if directory exists."""
    if os.path.isdir(dir_path):
        return True, "OK"
    return False, f"Directory not found: {dir_path}"


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Dynamic Path Construction (Scope Auto-Discovery)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


def module_dir(module: str) -> str:
    """Get directory path for a module.

    Example: module_dir('api') -> '/path/to/Python/api'
    """
    return os.path.join(ROOT, "Python", module)


def module_file(module: str, filename: str) -> str:
    """Get file path within a module.

    Example: module_file('api', 'config.py') -> '/path/to/Python/api/config.py'
    """
    return os.path.join(module_dir(module), filename)


def python_dir() -> str:
    """Get Python package directory."""
    return os.path.join(ROOT, "Python")


def validate_module_exists(module: str) -> bool:
    """Check if module exists in discovered modules."""
    return module in DISCOVERED_MODULES


def require_all(patterns: List[str], path: str) -> Tuple[bool, str]:
    missing = [p for p in patterns if not find_in_file(p, path)]
    if missing:
        return False, f"Missing patterns in {path}: {missing}"
    return True, "OK"


def require_any(patterns: List[str], path: str) -> Tuple[bool, str]:
    for p in patterns:
        if find_in_file(p, path):
            return True, "OK"
    return False, f"None of patterns found in {path}: {patterns}"


def policy_checks() -> List[Tuple[int, str, Callable[[], Tuple[bool, str]]]]:
    return [
        (
            1,
            "Configuration Sourcing (Zero-Heuristics)",
            lambda: require_all(
                [
                    r"FIELD_TO_SECTION_MAP",
                    r"Missing required config\.toml entry",
                ],
                module_file("api", "config.py"),
            ),
        ),
        (
            2,
            "Configuration Immutability (Locked Subsections)",
            lambda: require_all(
                [
                    r"float_precision",
                    r"jax_platform",
                    r"snapshot_path",
                    r"telemetry_buffer_maxlen",
                    r"credentials_vault_path",
                    r"telemetry_hash_interval_steps",
                    r"snapshot_integrity_hash_algorithm",
                    r"allowed_mutation_rate_per_hour",
                    r"max_deep_tuning_iterations",
                    r"checkpoint_path",
                    r"mutation_protocol_version",
                ],
                module_file("io", "config_mutation.py"),
            ),
        ),
        (
            3,
            "Validation Schema Enforcement",
            lambda: require_any(
                [r"validation_schema", r"ConfigMutationError", r"_validate_config"],
                module_file("io", "config_mutation.py"),
            ),
        ),
        (
            4,
            "Atomic Configuration Mutation Protocol",
            lambda: require_all(
                [r"O_EXCL", r"fsync", r"os\.replace", r"mutations\.log"],
                module_file("io", "config_mutation.py"),
            ),
        ),
        (
            5,
            "Mutation Rate Limiting and Rollback",
            lambda: require_any(
                [r"allowed_mutation_rate_per_hour", r"rollback", r"rmse"],
                module_file("io", "config_mutation.py"),
            ),
        ),
        (
            6,
            "Walk-Forward Validation (Causal)",
            lambda: require_any(
                [r"walk_forward", r"WalkForward"],
                module_file("core", "meta_optimizer.py"),
            ),
        ),
        (
            7,
            "CUSUM Dynamic Threshold with Kurtosis",
            lambda: find_in_dir(r"kurtosis|kappa|ln.*kappa", python_dir()),
        ),
        (
            8,
            "Signature Depth Constraint (M in [3,5])",
            lambda: find_in_dir(r"log_sig_depth|kernel_d_depth", python_dir()),
        ),
        (
            9,
            "Sinkhorn Epsilon Bounds",
            lambda: require_any(
                [r"sinkhorn_epsilon_min", r"epsilon_min", r"1e-4"],
                module_file("core", "sinkhorn.py"),
            ),
        ),
        (
            10,
            "CFL Condition for PIDE Schemes",
            lambda: find_in_dir(r"CFL|courant|c_safe", python_dir()),
        ),
        (
            11,
            "64-bit Precision Enablement",
            lambda: require_any(
                [r"jax_enable_x64", r"float64"],
                module_file("api", "config.py"),
            ),
        ),
        (
            12,
            "Stop-Gradient for Diagnostics",
            lambda: find_in_dir(r"stop_gradient", python_dir()),
        ),
        (
            13,
            "Kernel Purity and Statelessness",
            lambda: (
                not find_in_dir(r"\bprint\(|\bopen\(", module_dir("kernels"))[0],
                (
                    "No I/O in kernels"
                    if not find_in_dir(r"\bprint\(|\bopen\(", module_dir("kernels"))[0]
                    else "I/O found in kernels"
                ),
            ),
        ),
        (
            14,
            "Frozen Signal Detection and Recovery",
            lambda: require_all(
                [r"detect_frozen_signal", r"FrozenSignal"],
                module_file("io", "validators.py"),
            ),
        ),
        (
            15,
            "Catastrophic Outlier Rejection (20 sigma)",
            lambda: require_all(
                [r"detect_catastrophic_outlier", r"sigma_bound"],
                module_file("io", "validators.py"),
            ),
        ),
        (
            16,
            "Minimum Injection Frequency (Nyquist Soft Limit)",
            lambda: require_any(
                [r"besov_nyquist_interval", r"nyquist"],
                module_file("io", "validators.py"),
            ),
        ),
        (
            17,
            "Staleness Policy and Degraded Mode Recovery (TTL)",
            lambda: require_any(
                [r"staleness_ttl", r"degraded", r"TTL"],
                module_file("core", "orchestrator.py"),
            ),
        ),
        (
            18,
            "Secret Injection via Environment Variables",
            lambda: require_any(
                [r"getenv", r"MissingCredentialError"],
                module_file("io", "credentials.py"),
            ),
        ),
        (
            19,
            "Snapshot Integrity (SHA-256) and Validation",
            lambda: require_any(
                [r"sha256", r"SHA256"],
                module_file("io", "snapshots.py"),
            ),
        ),
        (
            20,
            "Non-Blocking Telemetry and I/O",
            lambda: require_any(
                [r"Thread", r"queue", r"deque"],
                module_file("io", "telemetry.py"),
            ),
        ),
        (
            21,
            "Hardware Parity Audit Hashes",
            lambda: require_any(
                [r"telemetry_hash_interval", r"parity"],
                module_file("io", "telemetry.py"),
            ),
        ),
        (
            22,
            "Emergency Mode on Singularities (Holder Threshold)",
            lambda: require_any(
                [r"holder_threshold", r"Huber", r"robust"],
                module_file("core", "orchestrator.py"),
            ),
        ),
        (
            23,
            "Entropy-Driven Capacity Expansion (DGM)",
            lambda: require_any(
                [r"entropy", r"capacity", r"dgm_max_capacity"],
                module_file("core", "orchestrator.py"),
            ),
        ),
        (
            24,
            "Dynamic Sinkhorn Regularization Coupling",
            lambda: require_any(
                [r"epsilon_t", r"sigma_t", r"sinkhorn"],
                module_file("core", "sinkhorn.py"),
            ),
        ),
        (
            25,
            "Entropy Window and Learning Rate Scaling (JKO)",
            lambda: require_any(
                [r"entropy_window", r"learning_rate"],
                module_file("core", "orchestrator.py"),
            ),
        ),
        (
            26,
            "Load Shedding (Kernel D Depth Set)",
            lambda: require_any(
                [r"warmup_kernel_d_load_shedding", r"kernel_d_load_shedding_depths"],
                module_file("api", "warmup.py"),
            ),
        ),
        (
            27,
            "Deterministic Execution and PRNG Configuration",
            lambda: find_in_dir(r"JAX_DETERMINISTIC_REDUCTIONS|jax_default_prng_impl|XLA_FLAGS", ROOT),
        ),
        (
            28,
            "Dependency Pinning (Exact Versions)",
            lambda: require_any(
                [r"=="],
                os.path.join(ROOT, "requirements.txt"),
            ),
        ),
        (
            29,
            "Five-Layer Architecture Enforcement",
            lambda: (
                all(os.path.isdir(module_dir(name)) for name in DISCOVERED_MODULES)
                and os.path.isdir(os.path.join(ROOT, "Test")),
                (
                    "OK"
                    if all(os.path.isdir(module_dir(name)) for name in DISCOVERED_MODULES)
                    else "Missing required layer directories"
                ),
            ),
        ),
        (
            30,
            "Snapshot Atomicity and Recovery (I/O)",
            lambda: require_any(
                [r"\.tmp", r"os\.replace", r"fsync"],
                module_file("io", "snapshots.py"),
            ),
        ),
        (
            31,
            "Meta-Optimization Checkpoint Integrity",
            lambda: require_any(
                [r"sha256", r"\.sha256"],
                module_file("core", "meta_optimizer.py"),
            ),
        ),
        (
            32,
            "TPE Resume Determinism",
            lambda: require_any(
                [r"rng_state", r"resume", r"trial_history"],
                module_file("core", "meta_optimizer.py"),
            ),
        ),
        (
            33,
            "Telemetry Flags and Alerts (Required Fields)",
            lambda: require_any(
                [r"degraded", r"emergency", r"regime_change", r"mode_collapse"],
                module_file("api", "schemas.py"),
            ),
        ),
        (
            36,
            "XLA No Host-Device Sync in Orchestrator",
            lambda: (
                not find_in_file(r"\.item\(\)", module_file("core", "orchestrator.py")),
                (
                    "OK"
                    if not find_in_file(r"\.item\(\)", module_file("core", "orchestrator.py"))
                    else "Host sync found in orchestrator"
                ),
            ),
        ),
        (
            37,
            "Vectorized vmap Parity",
            lambda: find_in_dir(r"vmap", python_dir()),
        ),
        (
            38,
            "JIT Cache Warmup Guarantees",
            lambda: require_any(
                [r"warmup_kernel_d_load_shedding"],
                module_file("api", "warmup.py"),
            ),
        ),
    ]


def run_checks() -> List[PolicyResult]:
    results: List[PolicyResult] = []
    for policy_id, name, checker in policy_checks():
        try:
            passed, details = checker()
        except Exception as exc:
            passed = False
            details = f"Exception: {exc}"
        results.append(PolicyResult(policy_id=policy_id, name=name, passed=passed, details=details))
    return results


def write_json_report(results: List[PolicyResult]) -> str:
    """Write JSON report (without timestamp in filename)."""
    os.makedirs(RESULTS_DIR, exist_ok=True)

    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
    passed = sum(1 for r in results if r.passed)
    failed = sum(1 for r in results if not r.passed)
    total = len(results)
    status = "PASS" if failed == 0 else "FAIL"

    modules = discover_modules(ROOT)
    files = []
    for module in modules:
        module_files = discover_module_files(module, ROOT)
        files.extend([f"Python/{module}/{name}" for name in module_files])

    issues = [f"Policy #{r.policy_id}: {r.name} - {r.details}" for r in results if not r.passed]

    payload = {
        "metadata": {
            "report_id": "code_alignment",
            "timestamp_utc": timestamp,
            "status": status,
            "source": "Test/scripts/code_alignment.py",
            "framework_version": "2.1.0",
            "notes": "Policy compliance checks",
        },
        "summary": {
            "title": "Execution Summary",
            "metrics": [
                {"label": "Total Policies", "value": total},
                {"label": "Passed", "value": passed},
                {"label": "Failed", "value": failed},
            ],
        },
        "scope": {
            "targets": {
                "folders": ["Python"],
                "files": sorted(set(files)),
                "modules": sorted(modules),
                "functions": [],
                "classes": [],
            }
        },
        "details": {
            "type": "table",
            "columns": ["Policy", "Status", "Details"],
            "rows": [
                [
                    f"#{r.policy_id} {r.name}",
                    "PASS" if r.passed else "FAIL",
                    r.details,
                ]
                for r in results
            ],
        },
        "issues": {
            "type": "list",
            "items": issues if issues else ["No findings"],
        },
        "extras": [],
    }

    json_path = os.path.join(RESULTS_DIR, "code_alignment_last.json")
    with open(json_path, "w", encoding="utf-8") as handle:
        json.dump(payload, handle, indent=2)

    return json_path


def main() -> int:
    """Run all policy compliance checks. Policies are defined in code, not loaded from file."""
    global USE_CACHE, CHANGED_FILES_CACHE

    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Policy compliance checker with incremental cache support")
    parser.add_argument(
        "--force-all",
        action="store_true",
        help="Process all files regardless of cache (use for CI/CD, pre-release, post-merge)",
    )
    parser.add_argument(
        "--no-cache",
        action="store_true",
        help="Disable cache system (same as --force-all but explicit)",
    )
    args = parser.parse_args()

    # Determine cache usage
    USE_CACHE = not (args.force_all or args.no_cache)

    # Initialize changed files cache if using cache
    if USE_CACHE and CACHE_AVAILABLE:
        try:
            CHANGED_FILES_CACHE = discover_changed_files(force_all=False)
            cache_info = get_cache_info()
            print(f"üìã Cache mode: INCREMENTAL ({len(CHANGED_FILES_CACHE)} changed files)")
            if cache_info.get("exists"):
                print(f"   Cache: {cache_info.get('cached_files', 0)} files tracked")
        except Exception as e:
            print(f"‚ö†Ô∏è  Cache error: {e}. Falling back to full scan.")
            USE_CACHE = False
            CHANGED_FILES_CACHE = None
    else:
        if args.force_all or args.no_cache:
            print("üìã Cache mode: FULL SCAN (--force-all)")
        else:
            print("üìã Cache mode: FULL SCAN (cache unavailable)")

        if CACHE_AVAILABLE:
            # Still populate cache for full scan to update it
            CHANGED_FILES_CACHE = discover_changed_files(force_all=True)

    print("")

    # Run checks
    results = run_checks()
    for result in results:
        status = "PASS" if result.passed else "FAIL"
        print(f"{status}: Policy #{result.policy_id} - {result.name}")
        if not result.passed:
            print(f"  - {result.details}")

    json_path = write_json_report(results)
    passed = sum(1 for r in results if r.passed)
    failed = sum(1 for r in results if not r.passed)
    total = len(results)
    print("")
    print("SUMMARY")
    print(f"Total: {total} | Passed: {passed} | Failed: {failed}")
    print(f"JSON Report: {json_path}")

    return 0 if failed == 0 else 1


if __name__ == "__main__":
    sys.exit(main())
